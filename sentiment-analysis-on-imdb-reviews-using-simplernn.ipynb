{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tensorflow keras","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-23T12:14:04.416486Z","iopub.execute_input":"2024-08-23T12:14:04.417319Z","iopub.status.idle":"2024-08-23T12:14:04.421995Z","shell.execute_reply.started":"2024-08-23T12:14:04.417277Z","shell.execute_reply":"2024-08-23T12:14:04.420985Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, Dense","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-23T12:14:04.423911Z","iopub.execute_input":"2024-08-23T12:14:04.424293Z","iopub.status.idle":"2024-08-23T12:14:07.811039Z","shell.execute_reply.started":"2024-08-23T12:14:04.424261Z","shell.execute_reply":"2024-08-23T12:14:07.810256Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load the Dataset","metadata":{}},{"cell_type":"code","source":"max_features = 10000 #vocab size\n(X_train, y_train),(X_test, y_test) = imdb.load_data(num_words = max_features)\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:07.812230Z","iopub.execute_input":"2024-08-23T12:14:07.812744Z","iopub.status.idle":"2024-08-23T12:14:12.792580Z","shell.execute_reply.started":"2024-08-23T12:14:07.812708Z","shell.execute_reply":"2024-08-23T12:14:12.791662Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((25000,), (25000,))"},"metadata":{}}]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:12.793937Z","iopub.execute_input":"2024-08-23T12:14:12.794350Z","iopub.status.idle":"2024-08-23T12:14:12.800465Z","shell.execute_reply.started":"2024-08-23T12:14:12.794305Z","shell.execute_reply":"2024-08-23T12:14:12.799477Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(25000,)"},"metadata":{}}]},{"cell_type":"code","source":"y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:12.804299Z","iopub.execute_input":"2024-08-23T12:14:12.804644Z","iopub.status.idle":"2024-08-23T12:14:12.810488Z","shell.execute_reply.started":"2024-08-23T12:14:12.804611Z","shell.execute_reply":"2024-08-23T12:14:12.809413Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(25000,)"},"metadata":{}}]},{"cell_type":"code","source":"X_train[0], y_train[0]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-23T12:14:12.811472Z","iopub.execute_input":"2024-08-23T12:14:12.811733Z","iopub.status.idle":"2024-08-23T12:14:12.822872Z","shell.execute_reply.started":"2024-08-23T12:14:12.811691Z","shell.execute_reply":"2024-08-23T12:14:12.821990Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"([1,\n  14,\n  22,\n  16,\n  43,\n  530,\n  973,\n  1622,\n  1385,\n  65,\n  458,\n  4468,\n  66,\n  3941,\n  4,\n  173,\n  36,\n  256,\n  5,\n  25,\n  100,\n  43,\n  838,\n  112,\n  50,\n  670,\n  2,\n  9,\n  35,\n  480,\n  284,\n  5,\n  150,\n  4,\n  172,\n  112,\n  167,\n  2,\n  336,\n  385,\n  39,\n  4,\n  172,\n  4536,\n  1111,\n  17,\n  546,\n  38,\n  13,\n  447,\n  4,\n  192,\n  50,\n  16,\n  6,\n  147,\n  2025,\n  19,\n  14,\n  22,\n  4,\n  1920,\n  4613,\n  469,\n  4,\n  22,\n  71,\n  87,\n  12,\n  16,\n  43,\n  530,\n  38,\n  76,\n  15,\n  13,\n  1247,\n  4,\n  22,\n  17,\n  515,\n  17,\n  12,\n  16,\n  626,\n  18,\n  2,\n  5,\n  62,\n  386,\n  12,\n  8,\n  316,\n  8,\n  106,\n  5,\n  4,\n  2223,\n  5244,\n  16,\n  480,\n  66,\n  3785,\n  33,\n  4,\n  130,\n  12,\n  16,\n  38,\n  619,\n  5,\n  25,\n  124,\n  51,\n  36,\n  135,\n  48,\n  25,\n  1415,\n  33,\n  6,\n  22,\n  12,\n  215,\n  28,\n  77,\n  52,\n  5,\n  14,\n  407,\n  16,\n  82,\n  2,\n  8,\n  4,\n  107,\n  117,\n  5952,\n  15,\n  256,\n  4,\n  2,\n  7,\n  3766,\n  5,\n  723,\n  36,\n  71,\n  43,\n  530,\n  476,\n  26,\n  400,\n  317,\n  46,\n  7,\n  4,\n  2,\n  1029,\n  13,\n  104,\n  88,\n  4,\n  381,\n  15,\n  297,\n  98,\n  32,\n  2071,\n  56,\n  26,\n  141,\n  6,\n  194,\n  7486,\n  18,\n  4,\n  226,\n  22,\n  21,\n  134,\n  476,\n  26,\n  480,\n  5,\n  144,\n  30,\n  5535,\n  18,\n  51,\n  36,\n  28,\n  224,\n  92,\n  25,\n  104,\n  4,\n  226,\n  65,\n  16,\n  38,\n  1334,\n  88,\n  12,\n  16,\n  283,\n  5,\n  16,\n  4472,\n  113,\n  103,\n  32,\n  15,\n  16,\n  5345,\n  19,\n  178,\n  32],\n 1)"},"metadata":{}}]},{"cell_type":"code","source":"# Inspect a sample\n\nsample_review = X_train[0]\nsample_label = y_train[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:12.823958Z","iopub.execute_input":"2024-08-23T12:14:12.824337Z","iopub.status.idle":"2024-08-23T12:14:12.829875Z","shell.execute_reply.started":"2024-08-23T12:14:12.824294Z","shell.execute_reply":"2024-08-23T12:14:12.828954Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(sample_review)\nprint(\"----------------------------------------------------------\")\nprint(sample_label)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:12.830951Z","iopub.execute_input":"2024-08-23T12:14:12.831270Z","iopub.status.idle":"2024-08-23T12:14:12.839595Z","shell.execute_reply.started":"2024-08-23T12:14:12.831239Z","shell.execute_reply":"2024-08-23T12:14:12.838765Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n----------------------------------------------------------\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"### Mapping of words index bacl to words(for understanding)\nword_index=imdb.get_word_index()\n#word_index\nreverse_word_index = {value: key for key, value in word_index.items()}\nreverse_word_index","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-23T12:14:12.840879Z","iopub.execute_input":"2024-08-23T12:14:12.841243Z","iopub.status.idle":"2024-08-23T12:14:12.950180Z","shell.execute_reply.started":"2024-08-23T12:14:12.841200Z","shell.execute_reply":"2024-08-23T12:14:12.949307Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{34701: 'fawn',\n 52006: 'tsukino',\n 52007: 'nunnery',\n 16816: 'sonja',\n 63951: 'vani',\n 1408: 'woods',\n 16115: 'spiders',\n 2345: 'hanging',\n 2289: 'woody',\n 52008: 'trawling',\n 52009: \"hold's\",\n 11307: 'comically',\n 40830: 'localized',\n 30568: 'disobeying',\n 52010: \"'royale\",\n 40831: \"harpo's\",\n 52011: 'canet',\n 19313: 'aileen',\n 52012: 'acurately',\n 52013: \"diplomat's\",\n 25242: 'rickman',\n 6746: 'arranged',\n 52014: 'rumbustious',\n 52015: 'familiarness',\n 52016: \"spider'\",\n 68804: 'hahahah',\n 52017: \"wood'\",\n 40833: 'transvestism',\n 34702: \"hangin'\",\n 2338: 'bringing',\n 40834: 'seamier',\n 34703: 'wooded',\n 52018: 'bravora',\n 16817: 'grueling',\n 1636: 'wooden',\n 16818: 'wednesday',\n 52019: \"'prix\",\n 34704: 'altagracia',\n 52020: 'circuitry',\n 11585: 'crotch',\n 57766: 'busybody',\n 52021: \"tart'n'tangy\",\n 14129: 'burgade',\n 52023: 'thrace',\n 11038: \"tom's\",\n 52025: 'snuggles',\n 29114: 'francesco',\n 52027: 'complainers',\n 52125: 'templarios',\n 40835: '272',\n 52028: '273',\n 52130: 'zaniacs',\n 34706: '275',\n 27631: 'consenting',\n 40836: 'snuggled',\n 15492: 'inanimate',\n 52030: 'uality',\n 11926: 'bronte',\n 4010: 'errors',\n 3230: 'dialogs',\n 52031: \"yomada's\",\n 34707: \"madman's\",\n 30585: 'dialoge',\n 52033: 'usenet',\n 40837: 'videodrome',\n 26338: \"kid'\",\n 52034: 'pawed',\n 30569: \"'girlfriend'\",\n 52035: \"'pleasure\",\n 52036: \"'reloaded'\",\n 40839: \"kazakos'\",\n 52037: 'rocque',\n 52038: 'mailings',\n 11927: 'brainwashed',\n 16819: 'mcanally',\n 52039: \"tom''\",\n 25243: 'kurupt',\n 21905: 'affiliated',\n 52040: 'babaganoosh',\n 40840: \"noe's\",\n 40841: 'quart',\n 359: 'kids',\n 5034: 'uplifting',\n 7093: 'controversy',\n 21906: 'kida',\n 23379: 'kidd',\n 52041: \"error'\",\n 52042: 'neurologist',\n 18510: 'spotty',\n 30570: 'cobblers',\n 9878: 'projection',\n 40842: 'fastforwarding',\n 52043: 'sters',\n 52044: \"eggar's\",\n 52045: 'etherything',\n 40843: 'gateshead',\n 34708: 'airball',\n 25244: 'unsinkable',\n 7180: 'stern',\n 52046: \"cervi's\",\n 40844: 'dnd',\n 11586: 'dna',\n 20598: 'insecurity',\n 52047: \"'reboot'\",\n 11037: 'trelkovsky',\n 52048: 'jaekel',\n 52049: 'sidebars',\n 52050: \"sforza's\",\n 17633: 'distortions',\n 52051: 'mutinies',\n 30602: 'sermons',\n 40846: '7ft',\n 52052: 'boobage',\n 52053: \"o'bannon's\",\n 23380: 'populations',\n 52054: 'chulak',\n 27633: 'mesmerize',\n 52055: 'quinnell',\n 10307: 'yahoo',\n 52057: 'meteorologist',\n 42577: 'beswick',\n 15493: 'boorman',\n 40847: 'voicework',\n 52058: \"ster'\",\n 22922: 'blustering',\n 52059: 'hj',\n 27634: 'intake',\n 5621: 'morally',\n 40849: 'jumbling',\n 52060: 'bowersock',\n 52061: \"'porky's'\",\n 16821: 'gershon',\n 40850: 'ludicrosity',\n 52062: 'coprophilia',\n 40851: 'expressively',\n 19500: \"india's\",\n 34710: \"post's\",\n 52063: 'wana',\n 5283: 'wang',\n 30571: 'wand',\n 25245: 'wane',\n 52321: 'edgeways',\n 34711: 'titanium',\n 40852: 'pinta',\n 178: 'want',\n 30572: 'pinto',\n 52065: 'whoopdedoodles',\n 21908: 'tchaikovsky',\n 2103: 'travel',\n 52066: \"'victory'\",\n 11928: 'copious',\n 22433: 'gouge',\n 52067: \"chapters'\",\n 6702: 'barbra',\n 30573: 'uselessness',\n 52068: \"wan'\",\n 27635: 'assimilated',\n 16116: 'petiot',\n 52069: 'most\\x85and',\n 3930: 'dinosaurs',\n 352: 'wrong',\n 52070: 'seda',\n 52071: 'stollen',\n 34712: 'sentencing',\n 40853: 'ouroboros',\n 40854: 'assimilates',\n 40855: 'colorfully',\n 27636: 'glenne',\n 52072: 'dongen',\n 4760: 'subplots',\n 52073: 'kiloton',\n 23381: 'chandon',\n 34713: \"effect'\",\n 27637: 'snugly',\n 40856: 'kuei',\n 9092: 'welcomed',\n 30071: 'dishonor',\n 52075: 'concurrence',\n 23382: 'stoicism',\n 14896: \"guys'\",\n 52077: \"beroemd'\",\n 6703: 'butcher',\n 40857: \"melfi's\",\n 30623: 'aargh',\n 20599: 'playhouse',\n 11308: 'wickedly',\n 1180: 'fit',\n 52078: 'labratory',\n 40859: 'lifeline',\n 1927: 'screaming',\n 4287: 'fix',\n 52079: 'cineliterate',\n 52080: 'fic',\n 52081: 'fia',\n 34714: 'fig',\n 52082: 'fmvs',\n 52083: 'fie',\n 52084: 'reentered',\n 30574: 'fin',\n 52085: 'doctresses',\n 52086: 'fil',\n 12606: 'zucker',\n 31931: 'ached',\n 52088: 'counsil',\n 52089: 'paterfamilias',\n 13885: 'songwriter',\n 34715: 'shivam',\n 9654: 'hurting',\n 299: 'effects',\n 52090: 'slauther',\n 52091: \"'flame'\",\n 52092: 'sommerset',\n 52093: 'interwhined',\n 27638: 'whacking',\n 52094: 'bartok',\n 8775: 'barton',\n 21909: 'frewer',\n 52095: \"fi'\",\n 6192: 'ingrid',\n 30575: 'stribor',\n 52096: 'approporiately',\n 52097: 'wobblyhand',\n 52098: 'tantalisingly',\n 52099: 'ankylosaurus',\n 17634: 'parasites',\n 52100: 'childen',\n 52101: \"jenkins'\",\n 52102: 'metafiction',\n 17635: 'golem',\n 40860: 'indiscretion',\n 23383: \"reeves'\",\n 57781: \"inamorata's\",\n 52104: 'brittannica',\n 7916: 'adapt',\n 30576: \"russo's\",\n 48246: 'guitarists',\n 10553: 'abbott',\n 40861: 'abbots',\n 17649: 'lanisha',\n 40863: 'magickal',\n 52105: 'mattter',\n 52106: \"'willy\",\n 34716: 'pumpkins',\n 52107: 'stuntpeople',\n 30577: 'estimate',\n 40864: 'ugghhh',\n 11309: 'gameplay',\n 52108: \"wern't\",\n 40865: \"n'sync\",\n 16117: 'sickeningly',\n 40866: 'chiara',\n 4011: 'disturbed',\n 40867: 'portmanteau',\n 52109: 'ineffectively',\n 82143: \"duchonvey's\",\n 37519: \"nasty'\",\n 1285: 'purpose',\n 52112: 'lazers',\n 28105: 'lightened',\n 52113: 'kaliganj',\n 52114: 'popularism',\n 18511: \"damme's\",\n 30578: 'stylistics',\n 52115: 'mindgaming',\n 46449: 'spoilerish',\n 52117: \"'corny'\",\n 34718: 'boerner',\n 6792: 'olds',\n 52118: 'bakelite',\n 27639: 'renovated',\n 27640: 'forrester',\n 52119: \"lumiere's\",\n 52024: 'gaskets',\n 884: 'needed',\n 34719: 'smight',\n 1297: 'master',\n 25905: \"edie's\",\n 40868: 'seeber',\n 52120: 'hiya',\n 52121: 'fuzziness',\n 14897: 'genesis',\n 12607: 'rewards',\n 30579: 'enthrall',\n 40869: \"'about\",\n 52122: \"recollection's\",\n 11039: 'mutilated',\n 52123: 'fatherlands',\n 52124: \"fischer's\",\n 5399: 'positively',\n 34705: '270',\n 34720: 'ahmed',\n 9836: 'zatoichi',\n 13886: 'bannister',\n 52127: 'anniversaries',\n 30580: \"helm's\",\n 52128: \"'work'\",\n 34721: 'exclaimed',\n 52129: \"'unfunny'\",\n 52029: '274',\n 544: 'feeling',\n 52131: \"wanda's\",\n 33266: 'dolan',\n 52133: '278',\n 52134: 'peacoat',\n 40870: 'brawny',\n 40871: 'mishra',\n 40872: 'worlders',\n 52135: 'protags',\n 52136: 'skullcap',\n 57596: 'dastagir',\n 5622: 'affairs',\n 7799: 'wholesome',\n 52137: 'hymen',\n 25246: 'paramedics',\n 52138: 'unpersons',\n 52139: 'heavyarms',\n 52140: 'affaire',\n 52141: 'coulisses',\n 40873: 'hymer',\n 52142: 'kremlin',\n 30581: 'shipments',\n 52143: 'pixilated',\n 30582: \"'00s\",\n 18512: 'diminishing',\n 1357: 'cinematic',\n 14898: 'resonates',\n 40874: 'simplify',\n 40875: \"nature'\",\n 40876: 'temptresses',\n 16822: 'reverence',\n 19502: 'resonated',\n 34722: 'dailey',\n 52144: '2\\x85',\n 27641: 'treize',\n 52145: 'majo',\n 21910: 'kiya',\n 52146: 'woolnough',\n 39797: 'thanatos',\n 35731: 'sandoval',\n 40879: 'dorama',\n 52147: \"o'shaughnessy\",\n 4988: 'tech',\n 32018: 'fugitives',\n 30583: 'teck',\n 76125: \"'e'\",\n 40881: 'doesn’t',\n 52149: 'purged',\n 657: 'saying',\n 41095: \"martians'\",\n 23418: 'norliss',\n 27642: 'dickey',\n 52152: 'dicker',\n 52153: \"'sependipity\",\n 8422: 'padded',\n 57792: 'ordell',\n 40882: \"sturges'\",\n 52154: 'independentcritics',\n 5745: 'tempted',\n 34724: \"atkinson's\",\n 25247: 'hounded',\n 52155: 'apace',\n 15494: 'clicked',\n 30584: \"'humor'\",\n 17177: \"martino's\",\n 52156: \"'supporting\",\n 52032: 'warmongering',\n 34725: \"zemeckis's\",\n 21911: 'lube',\n 52157: 'shocky',\n 7476: 'plate',\n 40883: 'plata',\n 40884: 'sturgess',\n 40885: \"nerds'\",\n 20600: 'plato',\n 34726: 'plath',\n 40886: 'platt',\n 52159: 'mcnab',\n 27643: 'clumsiness',\n 3899: 'altogether',\n 42584: 'massacring',\n 52160: 'bicenntinial',\n 40887: 'skaal',\n 14360: 'droning',\n 8776: 'lds',\n 21912: 'jaguar',\n 34727: \"cale's\",\n 1777: 'nicely',\n 4588: 'mummy',\n 18513: \"lot's\",\n 10086: 'patch',\n 50202: 'kerkhof',\n 52161: \"leader's\",\n 27644: \"'movie\",\n 52162: 'uncomfirmed',\n 40888: 'heirloom',\n 47360: 'wrangle',\n 52163: 'emotion\\x85',\n 52164: \"'stargate'\",\n 40889: 'pinoy',\n 40890: 'conchatta',\n 41128: 'broeke',\n 40891: 'advisedly',\n 17636: \"barker's\",\n 52166: 'descours',\n 772: 'lots',\n 9259: 'lotr',\n 9879: 'irs',\n 52167: 'lott',\n 40892: 'xvi',\n 34728: 'irk',\n 52168: 'irl',\n 6887: 'ira',\n 21913: 'belzer',\n 52169: 'irc',\n 27645: 'ire',\n 40893: 'requisites',\n 7693: 'discipline',\n 52961: 'lyoko',\n 11310: 'extend',\n 873: 'nature',\n 52170: \"'dickie'\",\n 40894: 'optimist',\n 30586: 'lapping',\n 3900: 'superficial',\n 52171: 'vestment',\n 2823: 'extent',\n 52172: 'tendons',\n 52173: \"heller's\",\n 52174: 'quagmires',\n 52175: 'miyako',\n 20601: 'moocow',\n 52176: \"coles'\",\n 40895: 'lookit',\n 52177: 'ravenously',\n 40896: 'levitating',\n 52178: 'perfunctorily',\n 30587: 'lookin',\n 40898: \"lot'\",\n 52179: 'lookie',\n 34870: 'fearlessly',\n 52181: 'libyan',\n 40899: 'fondles',\n 35714: 'gopher',\n 40901: 'wearying',\n 52182: \"nz's\",\n 27646: 'minuses',\n 52183: 'puposelessly',\n 52184: 'shandling',\n 31268: 'decapitates',\n 11929: 'humming',\n 40902: \"'nother\",\n 21914: 'smackdown',\n 30588: 'underdone',\n 40903: 'frf',\n 52185: 'triviality',\n 25248: 'fro',\n 8777: 'bothers',\n 52186: \"'kensington\",\n 73: 'much',\n 34730: 'muco',\n 22615: 'wiseguy',\n 27648: \"richie's\",\n 40904: 'tonino',\n 52187: 'unleavened',\n 11587: 'fry',\n 40905: \"'tv'\",\n 40906: 'toning',\n 14361: 'obese',\n 30589: 'sensationalized',\n 40907: 'spiv',\n 6259: 'spit',\n 7364: 'arkin',\n 21915: 'charleton',\n 16823: 'jeon',\n 21916: 'boardroom',\n 4989: 'doubts',\n 3084: 'spin',\n 53083: 'hepo',\n 27649: 'wildcat',\n 10584: 'venoms',\n 52191: 'misconstrues',\n 18514: 'mesmerising',\n 40908: 'misconstrued',\n 52192: 'rescinds',\n 52193: 'prostrate',\n 40909: 'majid',\n 16479: 'climbed',\n 34731: 'canoeing',\n 52195: 'majin',\n 57804: 'animie',\n 40910: 'sylke',\n 14899: 'conditioned',\n 40911: 'waddell',\n 52196: '3\\x85',\n 41188: 'hyperdrive',\n 34732: 'conditioner',\n 53153: 'bricklayer',\n 2576: 'hong',\n 52198: 'memoriam',\n 30592: 'inventively',\n 25249: \"levant's\",\n 20638: 'portobello',\n 52200: 'remand',\n 19504: 'mummified',\n 27650: 'honk',\n 19505: 'spews',\n 40912: 'visitations',\n 52201: 'mummifies',\n 25250: 'cavanaugh',\n 23385: 'zeon',\n 40913: \"jungle's\",\n 34733: 'viertel',\n 27651: 'frenchmen',\n 52202: 'torpedoes',\n 52203: 'schlessinger',\n 34734: 'torpedoed',\n 69876: 'blister',\n 52204: 'cinefest',\n 34735: 'furlough',\n 52205: 'mainsequence',\n 40914: 'mentors',\n 9094: 'academic',\n 20602: 'stillness',\n 40915: 'academia',\n 52206: 'lonelier',\n 52207: 'nibby',\n 52208: \"losers'\",\n 40916: 'cineastes',\n 4449: 'corporate',\n 40917: 'massaging',\n 30593: 'bellow',\n 19506: 'absurdities',\n 53241: 'expetations',\n 40918: 'nyfiken',\n 75638: 'mehras',\n 52209: 'lasse',\n 52210: 'visability',\n 33946: 'militarily',\n 52211: \"elder'\",\n 19023: 'gainsbourg',\n 20603: 'hah',\n 13420: 'hai',\n 34736: 'haj',\n 25251: 'hak',\n 4311: 'hal',\n 4892: 'ham',\n 53259: 'duffer',\n 52213: 'haa',\n 66: 'had',\n 11930: 'advancement',\n 16825: 'hag',\n 25252: \"hand'\",\n 13421: 'hay',\n 20604: 'mcnamara',\n 52214: \"mozart's\",\n 30731: 'duffel',\n 30594: 'haq',\n 13887: 'har',\n 44: 'has',\n 2401: 'hat',\n 40919: 'hav',\n 30595: 'haw',\n 52215: 'figtings',\n 15495: 'elders',\n 52216: 'underpanted',\n 52217: 'pninson',\n 27652: 'unequivocally',\n 23673: \"barbara's\",\n 52219: \"bello'\",\n 12997: 'indicative',\n 40920: 'yawnfest',\n 52220: 'hexploitation',\n 52221: \"loder's\",\n 27653: 'sleuthing',\n 32622: \"justin's\",\n 52222: \"'ball\",\n 52223: \"'summer\",\n 34935: \"'demons'\",\n 52225: \"mormon's\",\n 34737: \"laughton's\",\n 52226: 'debell',\n 39724: 'shipyard',\n 30597: 'unabashedly',\n 40401: 'disks',\n 2290: 'crowd',\n 10087: 'crowe',\n 56434: \"vancouver's\",\n 34738: 'mosques',\n 6627: 'crown',\n 52227: 'culpas',\n 27654: 'crows',\n 53344: 'surrell',\n 52229: 'flowless',\n 52230: 'sheirk',\n 40923: \"'three\",\n 52231: \"peterson'\",\n 52232: 'ooverall',\n 40924: 'perchance',\n 1321: 'bottom',\n 53363: 'chabert',\n 52233: 'sneha',\n 13888: 'inhuman',\n 52234: 'ichii',\n 52235: 'ursla',\n 30598: 'completly',\n 40925: 'moviedom',\n 52236: 'raddick',\n 51995: 'brundage',\n 40926: 'brigades',\n 1181: 'starring',\n 52237: \"'goal'\",\n 52238: 'caskets',\n 52239: 'willcock',\n 52240: \"threesome's\",\n 52241: \"mosque'\",\n 52242: \"cover's\",\n 17637: 'spaceships',\n 40927: 'anomalous',\n 27655: 'ptsd',\n 52243: 'shirdan',\n 21962: 'obscenity',\n 30599: 'lemmings',\n 30600: 'duccio',\n 52244: \"levene's\",\n 52245: \"'gorby'\",\n 25255: \"teenager's\",\n 5340: 'marshall',\n 9095: 'honeymoon',\n 3231: 'shoots',\n 12258: 'despised',\n 52246: 'okabasho',\n 8289: 'fabric',\n 18515: 'cannavale',\n 3537: 'raped',\n 52247: \"tutt's\",\n 17638: 'grasping',\n 18516: 'despises',\n 40928: \"thief's\",\n 8926: 'rapes',\n 52248: 'raper',\n 27656: \"eyre'\",\n 52249: 'walchek',\n 23386: \"elmo's\",\n 40929: 'perfumes',\n 21918: 'spurting',\n 52250: \"exposition'\\x85\",\n 52251: 'denoting',\n 34740: 'thesaurus',\n 40930: \"shoot'\",\n 49759: 'bonejack',\n 52253: 'simpsonian',\n 30601: 'hebetude',\n 34741: \"hallow's\",\n 52254: 'desperation\\x85',\n 34742: 'incinerator',\n 10308: 'congratulations',\n 52255: 'humbled',\n 5924: \"else's\",\n 40845: 'trelkovski',\n 52256: \"rape'\",\n 59386: \"'chapters'\",\n 52257: '1600s',\n 7253: 'martian',\n 25256: 'nicest',\n 52259: 'eyred',\n 9457: 'passenger',\n 6041: 'disgrace',\n 52260: 'moderne',\n 5120: 'barrymore',\n 52261: 'yankovich',\n 40931: 'moderns',\n 52262: 'studliest',\n 52263: 'bedsheet',\n 14900: 'decapitation',\n 52264: 'slurring',\n 52265: \"'nunsploitation'\",\n 34743: \"'character'\",\n 9880: 'cambodia',\n 52266: 'rebelious',\n 27657: 'pasadena',\n 40932: 'crowne',\n 52267: \"'bedchamber\",\n 52268: 'conjectural',\n 52269: 'appologize',\n 52270: 'halfassing',\n 57816: 'paycheque',\n 20606: 'palms',\n 52271: \"'islands\",\n 40933: 'hawked',\n 21919: 'palme',\n 40934: 'conservatively',\n 64007: 'larp',\n 5558: 'palma',\n 21920: 'smelling',\n 12998: 'aragorn',\n 52272: 'hawker',\n 52273: 'hawkes',\n 3975: 'explosions',\n 8059: 'loren',\n 52274: \"pyle's\",\n 6704: 'shootout',\n 18517: \"mike's\",\n 52275: \"driscoll's\",\n 40935: 'cogsworth',\n 52276: \"britian's\",\n 34744: 'childs',\n 52277: \"portrait's\",\n 3626: 'chain',\n 2497: 'whoever',\n 52278: 'puttered',\n 52279: 'childe',\n 52280: 'maywether',\n 3036: 'chair',\n 52281: \"rance's\",\n 34745: 'machu',\n 4517: 'ballet',\n 34746: 'grapples',\n 76152: 'summerize',\n 30603: 'freelance',\n 52283: \"andrea's\",\n 52284: '\\x91very',\n 45879: 'coolidge',\n 18518: 'mache',\n 52285: 'balled',\n 40937: 'grappled',\n 18519: 'macha',\n 21921: 'underlining',\n 5623: 'macho',\n 19507: 'oversight',\n 25257: 'machi',\n 11311: 'verbally',\n 21922: 'tenacious',\n 40938: 'windshields',\n 18557: 'paychecks',\n 3396: 'jerk',\n 11931: \"good'\",\n 34748: 'prancer',\n 21923: 'prances',\n 52286: 'olympus',\n 21924: 'lark',\n 10785: 'embark',\n 7365: 'gloomy',\n 52287: 'jehaan',\n 52288: 'turaqui',\n 20607: \"child'\",\n 2894: 'locked',\n 52289: 'pranced',\n 2588: 'exact',\n 52290: 'unattuned',\n 783: 'minute',\n 16118: 'skewed',\n 40940: 'hodgins',\n 34749: 'skewer',\n 52291: 'think\\x85',\n 38765: 'rosenstein',\n 52292: 'helmit',\n 34750: 'wrestlemanias',\n 16826: 'hindered',\n 30604: \"martha's\",\n 52293: 'cheree',\n 52294: \"pluckin'\",\n 40941: 'ogles',\n 11932: 'heavyweight',\n 82190: 'aada',\n 11312: 'chopping',\n 61534: 'strongboy',\n 41342: 'hegemonic',\n 40942: 'adorns',\n 41346: 'xxth',\n 34751: 'nobuhiro',\n 52298: 'capitães',\n 52299: 'kavogianni',\n 13422: 'antwerp',\n 6538: 'celebrated',\n 52300: 'roarke',\n 40943: 'baggins',\n 31270: 'cheeseburgers',\n 52301: 'matras',\n 52302: \"nineties'\",\n 52303: \"'craig'\",\n 12999: 'celebrates',\n 3383: 'unintentionally',\n 14362: 'drafted',\n 52304: 'climby',\n 52305: '303',\n 18520: 'oldies',\n 9096: 'climbs',\n 9655: 'honour',\n 34752: 'plucking',\n 30074: '305',\n 5514: 'address',\n 40944: 'menjou',\n 42592: \"'freak'\",\n 19508: 'dwindling',\n 9458: 'benson',\n 52307: 'white’s',\n 40945: 'shamelessness',\n 21925: 'impacted',\n 52308: 'upatz',\n 3840: 'cusack',\n 37567: \"flavia's\",\n 52309: 'effette',\n 34753: 'influx',\n 52310: 'boooooooo',\n 52311: 'dimitrova',\n 13423: 'houseman',\n 25259: 'bigas',\n 52312: 'boylen',\n 52313: 'phillipenes',\n 40946: 'fakery',\n 27658: \"grandpa's\",\n 27659: 'darnell',\n 19509: 'undergone',\n 52315: 'handbags',\n 21926: 'perished',\n 37778: 'pooped',\n 27660: 'vigour',\n 3627: 'opposed',\n 52316: 'etude',\n 11799: \"caine's\",\n 52317: 'doozers',\n 34754: 'photojournals',\n 52318: 'perishes',\n 34755: 'constrains',\n 40948: 'migenes',\n 30605: 'consoled',\n 16827: 'alastair',\n 52319: 'wvs',\n 52320: 'ooooooh',\n 34756: 'approving',\n 40949: 'consoles',\n 52064: 'disparagement',\n 52322: 'futureistic',\n 52323: 'rebounding',\n 52324: \"'date\",\n 52325: 'gregoire',\n 21927: 'rutherford',\n 34757: 'americanised',\n 82196: 'novikov',\n 1042: 'following',\n 34758: 'munroe',\n 52326: \"morita'\",\n 52327: 'christenssen',\n 23106: 'oatmeal',\n 25260: 'fossey',\n 40950: 'livered',\n 13000: 'listens',\n 76164: \"'marci\",\n 52330: \"otis's\",\n 23387: 'thanking',\n 16019: 'maude',\n 34759: 'extensions',\n 52332: 'ameteurish',\n 52333: \"commender's\",\n 27661: 'agricultural',\n 4518: 'convincingly',\n 17639: 'fueled',\n 54014: 'mahattan',\n 40952: \"paris's\",\n 52336: 'vulkan',\n 52337: 'stapes',\n 52338: 'odysessy',\n 12259: 'harmon',\n 4252: 'surfing',\n 23494: 'halloran',\n 49580: 'unbelieveably',\n 52339: \"'offed'\",\n 30607: 'quadrant',\n 19510: 'inhabiting',\n 34760: 'nebbish',\n 40953: 'forebears',\n 34761: 'skirmish',\n 52340: 'ocassionally',\n 52341: \"'resist\",\n 21928: 'impactful',\n 52342: 'spicier',\n 40954: 'touristy',\n 52343: \"'football'\",\n 40955: 'webpage',\n 52345: 'exurbia',\n 52346: 'jucier',\n 14901: 'professors',\n 34762: 'structuring',\n 30608: 'jig',\n 40956: 'overlord',\n 25261: 'disconnect',\n 82201: 'sniffle',\n 40957: 'slimeball',\n 40958: 'jia',\n 16828: 'milked',\n 40959: 'banjoes',\n 1237: 'jim',\n 52348: 'workforces',\n 52349: 'jip',\n 52350: 'rotweiller',\n 34763: 'mundaneness',\n 52351: \"'ninja'\",\n 11040: \"dead'\",\n 40960: \"cipriani's\",\n 20608: 'modestly',\n 52352: \"professor'\",\n 40961: 'shacked',\n 34764: 'bashful',\n 23388: 'sorter',\n 16120: 'overpowering',\n 18521: 'workmanlike',\n 27662: 'henpecked',\n 18522: 'sorted',\n 52354: \"jōb's\",\n 52355: \"'always\",\n 34765: \"'baptists\",\n 52356: 'dreamcatchers',\n 52357: \"'silence'\",\n 21929: 'hickory',\n 52358: 'fun\\x97yet',\n 52359: 'breakumentary',\n 15496: 'didn',\n 52360: 'didi',\n 52361: 'pealing',\n 40962: 'dispite',\n 25262: \"italy's\",\n 21930: 'instability',\n 6539: 'quarter',\n 12608: 'quartet',\n 52362: 'padmé',\n 52363: \"'bleedmedry\",\n 52364: 'pahalniuk',\n 52365: 'honduras',\n 10786: 'bursting',\n 41465: \"pablo's\",\n 52367: 'irremediably',\n 40963: 'presages',\n 57832: 'bowlegged',\n 65183: 'dalip',\n 6260: 'entering',\n 76172: 'newsradio',\n 54150: 'presaged',\n 27663: \"giallo's\",\n 40964: 'bouyant',\n 52368: 'amerterish',\n 18523: 'rajni',\n 30610: 'leeves',\n 34767: 'macauley',\n 612: 'seriously',\n 52369: 'sugercoma',\n 52370: 'grimstead',\n 52371: \"'fairy'\",\n 30611: 'zenda',\n 52372: \"'twins'\",\n 17640: 'realisation',\n 27664: 'highsmith',\n 7817: 'raunchy',\n 40965: 'incentives',\n 52374: 'flatson',\n 35097: 'snooker',\n 16829: 'crazies',\n 14902: 'crazier',\n 7094: 'grandma',\n 52375: 'napunsaktha',\n 30612: 'workmanship',\n 52376: 'reisner',\n 61306: \"sanford's\",\n 52377: '\\x91doña',\n 6108: 'modest',\n 19153: \"everything's\",\n 40966: 'hamer',\n 52379: \"couldn't'\",\n 13001: 'quibble',\n 52380: 'socking',\n 21931: 'tingler',\n 52381: 'gutman',\n 40967: 'lachlan',\n 52382: 'tableaus',\n 52383: 'headbanger',\n 2847: 'spoken',\n 34768: 'cerebrally',\n 23490: \"'road\",\n 21932: 'tableaux',\n 40968: \"proust's\",\n 40969: 'periodical',\n 52385: \"shoveller's\",\n 25263: 'tamara',\n 17641: 'affords',\n 3249: 'concert',\n 87955: \"yara's\",\n 52386: 'someome',\n 8424: 'lingering',\n 41511: \"abraham's\",\n 34769: 'beesley',\n 34770: 'cherbourg',\n 28624: 'kagan',\n 9097: 'snatch',\n 9260: \"miyazaki's\",\n 25264: 'absorbs',\n 40970: \"koltai's\",\n 64027: 'tingled',\n 19511: 'crossroads',\n 16121: 'rehab',\n 52389: 'falworth',\n 52390: 'sequals',\n ...}"},"metadata":{}}]},{"cell_type":"code","source":"decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in sample_review])\ndecoded_review","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:12.951432Z","iopub.execute_input":"2024-08-23T12:14:12.951754Z","iopub.status.idle":"2024-08-23T12:14:12.957971Z","shell.execute_reply.started":"2024-08-23T12:14:12.951721Z","shell.execute_reply":"2024-08-23T12:14:12.957117Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""},"metadata":{}}]},{"cell_type":"code","source":"max_len=500","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:12.959156Z","iopub.execute_input":"2024-08-23T12:14:12.959430Z","iopub.status.idle":"2024-08-23T12:14:12.965637Z","shell.execute_reply.started":"2024-08-23T12:14:12.959388Z","shell.execute_reply":"2024-08-23T12:14:12.964790Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import sequence\n\n\nX_train=sequence.pad_sequences(X_train,maxlen=max_len)\nX_test = sequence.pad_sequences(X_test, maxlen=max_len)\nX_train","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:12.966730Z","iopub.execute_input":"2024-08-23T12:14:12.967039Z","iopub.status.idle":"2024-08-23T12:14:13.974751Z","shell.execute_reply.started":"2024-08-23T12:14:12.966982Z","shell.execute_reply":"2024-08-23T12:14:13.973780Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[   0,    0,    0, ...,   19,  178,   32],\n       [   0,    0,    0, ...,   16,  145,   95],\n       [   0,    0,    0, ...,    7,  129,  113],\n       ...,\n       [   0,    0,    0, ...,    4, 3586,    2],\n       [   0,    0,    0, ...,   12,    9,   23],\n       [   0,    0,    0, ...,  204,  131,    9]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train Simple RNN","metadata":{}},{"cell_type":"markdown","source":"### Embedding Code Explanation:\n```python\nmodel.add(Embedding(max_features, 128, input_length=max_len))\n```\n\n- **`model.add(...)`:** \n  - This adds a new layer to the model. The model here is typically a sequential model, which means layers are added one after the other.\n\n- **`Embedding(...)`:**\n  - The `Embedding` layer is a type of layer that converts positive integers (which represent words) into dense vectors of fixed size. This is a crucial layer in Natural Language Processing (NLP) models, where words or tokens are represented as vectors.\n\n### Parameters:\n1. **`max_features`:**\n   - This parameter specifies the size of the vocabulary or the maximum number of unique words (tokens) that the model will consider. \n   - For example, if `max_features` is 10,000, the model will only consider the top 10,000 most frequent words in your dataset.\n\n2. **`128`:**\n   - This is the `output_dim`, which specifies the dimensionality of the embedding vectors. \n   - Each word or token in your vocabulary will be represented by a vector of size 128. This is a hyperparameter you can tune based on the complexity of your problem.\n\n3. **`input_length = max_len`:**\n   - This parameter specifies the length of the input sequences. \n   - For example, if your input sequences are padded or truncated to a length of 100 words, `max_len` would be 100.\n   - This ensures that the output of the `Embedding` layer is a 2D tensor with shape `(batch_size, max_len, 128)`.\n\n### What the Embedding Layer Does:\n- **Input:** The `Embedding` layer expects input as sequences of integers, where each integer corresponds to a word index in the vocabulary.\n  - Example: A sequence `[4, 3, 2, 1]` where each number represents a specific word in the vocabulary.\n\n- **Output:** The output is a 3D tensor where each word index is replaced by its corresponding dense vector.\n  - If `input_length = 100` and `output_dim = 128`, each input sequence is converted into a 2D array of shape `(100, 128)`. For a batch of sequences, the output shape will be `(batch_size, 100, 128)`.\n\n### Example:\nAssume `max_features = 10,000`, `max_len = 100`, and `output_dim = 128`.\n\n- **Input:** A batch of sequences with each sequence containing up to 100 integers (representing word indices).\n- **Output:** A batch of sequences where each word index is replaced by a 128-dimensional vector, resulting in a shape of `(batch_size, 100, 128)`.\n\n### Purpose in NLP:\nThe `Embedding` layer allows the model to learn word embeddings from the data during training. These embeddings capture semantic information about the words, meaning words with similar contexts tend to have similar vector representations.\n\nThis layer is often the first layer in a model designed to process text data, feeding the dense vector representations into subsequent layers, such as LSTMs, GRUs, or CNNs, depending on the architecture.","metadata":{}},{"cell_type":"code","source":"## Train Simple RNN\nmodel=Sequential()\nmodel.add(Embedding(max_features,128,input_length=max_len)) ## Embedding Layers\nmodel.add(SimpleRNN(128,activation='relu'))\nmodel.add(Dense(1,activation=\"sigmoid\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:13.975922Z","iopub.execute_input":"2024-08-23T12:14:13.976263Z","iopub.status.idle":"2024-08-23T12:14:14.541832Z","shell.execute_reply.started":"2024-08-23T12:14:13.976229Z","shell.execute_reply":"2024-08-23T12:14:14.540579Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 128, input_length=max_len))  # Embedding Layer\nmodel.add(SimpleRNN(128, activation='relu'))  # SimpleRNN Layer\nmodel.add(Dense(1, activation=\"sigmoid\"))  # Dense Layer\n\n# Now call model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:14.546353Z","iopub.execute_input":"2024-08-23T12:14:14.546642Z","iopub.status.idle":"2024-08-23T12:14:14.571541Z","shell.execute_reply.started":"2024-08-23T12:14:14.546612Z","shell.execute_reply":"2024-08-23T12:14:14.570717Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## compile:\n```python\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n```\n\n### Parameters:\n\n1. **`optimizer='adam'`:**\n   - **Optimizer**: The optimizer is an algorithm that adjusts the weights of the neural network to minimize the loss during training. It controls how the model learns.\n   - **Adam**: This is a popular optimizer called \"Adam\" (short for Adaptive Moment Estimation). It's a combination of two other optimizers, AdaGrad and RMSProp, and it's known for being efficient and performing well on a wide range of problems. Adam automatically adjusts the learning rate during training, making it easier to train the model.\n\n2. **`loss='binary_crossentropy'`:**\n   - **Loss Function**: The loss function is what the model tries to minimize during training. It measures how well or poorly the model is performing.\n   - **Binary Crossentropy**: This is a specific type of loss function used for binary classification problems, where the output is either 0 or 1. It measures the difference between the predicted probability (the model's guess) and the actual label (the true answer). The goal is to make the predicted probabilities as close as possible to the actual labels.\n\n3. **`metrics=['accuracy']`:**\n   - **Metrics**: Metrics are used to evaluate the performance of the model. While the loss function is used to optimize the model during training, metrics are used to judge how well the model is performing in a more human-understandable way.\n   - **Accuracy**: Accuracy is a common metric for classification tasks. It calculates the percentage of correctly predicted labels out of all predictions made. If the model predicts the correct label for 90 out of 100 examples, the accuracy would be 90%.\n\n### Summary:\n- **Optimizer (`'adam'`)**: Controls how the model updates its weights to minimize the loss.\n- **Loss Function (`'binary_crossentropy'`)**: Measures how far off the model's predictions are from the actual labels, specifically for binary classification.\n- **Metrics (`'accuracy'`)**: Tracks how often the model's predictions are correct during training and evaluation.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:14.572510Z","iopub.execute_input":"2024-08-23T12:14:14.572786Z","iopub.status.idle":"2024-08-23T12:14:14.589328Z","shell.execute_reply.started":"2024-08-23T12:14:14.572755Z","shell.execute_reply":"2024-08-23T12:14:14.588452Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create an instance of EarlyStoppping Callback\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearlystopping=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\nearlystopping","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:14.590554Z","iopub.execute_input":"2024-08-23T12:14:14.591343Z","iopub.status.idle":"2024-08-23T12:14:14.598147Z","shell.execute_reply.started":"2024-08-23T12:14:14.591298Z","shell.execute_reply":"2024-08-23T12:14:14.597213Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.early_stopping.EarlyStopping at 0x7f56d6b7e7a0>"},"metadata":{}}]},{"cell_type":"code","source":"# Train the model with early stopping\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    epochs = 20,\n    batch_size = 32,\n    validation_split = 0.2,\n    callbacks = earlystopping\n)\n\nhistory","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:14:14.599425Z","iopub.execute_input":"2024-08-23T12:14:14.600256Z","iopub.status.idle":"2024-08-23T12:18:57.345367Z","shell.execute_reply.started":"2024-08-23T12:14:14.600209Z","shell.execute_reply":"2024-08-23T12:18:57.344400Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1724415256.082666    1619 service.cc:145] XLA service 0x7f557c0033f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1724415256.082730    1619 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1724415256.082736    1619 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  3/625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.5434 - loss: 0.6927","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1724415260.586254    1619 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.6292 - loss: 0.9826 - val_accuracy: 0.6468 - val_loss: 0.6158\nEpoch 2/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.6559 - loss: 29.9358 - val_accuracy: 0.6612 - val_loss: 0.6000\nEpoch 3/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.6702 - loss: 2.1527 - val_accuracy: 0.6698 - val_loss: 0.5955\nEpoch 4/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.7482 - loss: 0.5239 - val_accuracy: 0.7018 - val_loss: 0.5545\nEpoch 5/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.7410 - loss: 69.1988 - val_accuracy: 0.7746 - val_loss: 0.4707\nEpoch 6/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.8799 - loss: 0.3102 - val_accuracy: 0.8096 - val_loss: 0.4220\nEpoch 7/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.9054 - loss: 122.4947 - val_accuracy: 0.6906 - val_loss: 0.5956\nEpoch 8/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.8594 - loss: 0.3291 - val_accuracy: 0.8064 - val_loss: 0.4435\nEpoch 9/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.9231 - loss: 0.2033 - val_accuracy: 0.8222 - val_loss: 0.4379\nEpoch 10/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.9471 - loss: 0.1541 - val_accuracy: 0.8288 - val_loss: 0.4586\nEpoch 11/20\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.9545 - loss: 0.1374 - val_accuracy: 0.8226 - val_loss: 0.4904\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7f561e8a0f70>"},"metadata":{}}]},{"cell_type":"code","source":"# Save model file\nmodel.save(\"simple_rnn_imdb.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:57.346725Z","iopub.execute_input":"2024-08-23T12:18:57.347064Z","iopub.status.idle":"2024-08-23T12:18:57.401009Z","shell.execute_reply.started":"2024-08-23T12:18:57.347026Z","shell.execute_reply":"2024-08-23T12:18:57.400055Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# Load the IMDB dataset word index\nword_index = imdb.get_word_index()\nreverse_word_index = {value: key for key, value in word_index.items()}","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:57.402218Z","iopub.execute_input":"2024-08-23T12:18:57.402506Z","iopub.status.idle":"2024-08-23T12:18:57.463284Z","shell.execute_reply.started":"2024-08-23T12:18:57.402476Z","shell.execute_reply":"2024-08-23T12:18:57.462311Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained model with ReLU activation\nfrom tensorflow.keras.models import load_model\nmodel = load_model('/kaggle/working/simple_rnn_imdb.h5')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:57.464450Z","iopub.execute_input":"2024-08-23T12:18:57.464736Z","iopub.status.idle":"2024-08-23T12:18:57.619925Z","shell.execute_reply.started":"2024-08-23T12:18:57.464706Z","shell.execute_reply":"2024-08-23T12:18:57.619050Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │     \u001b[38;5;34m1,280,000\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │           \u001b[38;5;34m129\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,027\u001b[0m (5.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,027</span> (5.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,025\u001b[0m (5.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,025</span> (5.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model.get_weights()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-23T12:18:57.621126Z","iopub.execute_input":"2024-08-23T12:18:57.621427Z","iopub.status.idle":"2024-08-23T12:18:57.637015Z","shell.execute_reply.started":"2024-08-23T12:18:57.621394Z","shell.execute_reply":"2024-08-23T12:18:57.636139Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[array([[-3.5104254e-01, -6.8887001e-01, -3.0445252e-04, ...,\n         -1.4798352e+00,  1.5217998e+00,  7.0140737e-01],\n        [ 6.6881001e-02,  9.8892577e-02, -1.5148056e-02, ...,\n         -4.3640610e-02, -2.6646830e-02, -4.6602005e-04],\n        [ 1.0248376e-01,  1.1923243e-01,  1.3193871e-02, ...,\n         -2.0473291e-01, -4.5470055e-02, -1.5189956e-01],\n        ...,\n        [ 2.4755094e-02,  8.6530931e-03,  2.9401893e-03, ...,\n         -4.3557230e-02,  1.6845986e-01,  9.5465504e-02],\n        [-1.2162362e-02,  5.5011269e-02, -4.1814171e-02, ...,\n          1.3034658e-01, -1.1220130e-01, -1.4685093e-01],\n        [-6.7589954e-02, -1.1406129e-01,  1.0422938e-01, ...,\n          5.9046723e-02,  1.6084655e-01,  5.0558634e-02]], dtype=float32),\n array([[-0.21568975, -0.00555647,  0.00494833, ..., -0.00412877,\n         -0.08309796, -0.04685008],\n        [ 0.06287009,  0.10813892, -0.14880838, ..., -0.12021381,\n         -0.15270306, -0.11986981],\n        [-0.02466212,  0.14061598, -0.15718928, ..., -0.08039904,\n         -0.08115252, -0.16489598],\n        ...,\n        [ 0.17500895,  0.0232726 , -0.14155121, ...,  0.07668317,\n         -0.00149088,  0.03300368],\n        [-0.15572655, -0.01100984, -0.13012342, ..., -0.12003659,\n         -0.15739745, -0.0182293 ],\n        [ 0.16805401, -0.03230872,  0.12271266, ...,  0.15345418,\n          0.18234894,  0.08673899]], dtype=float32),\n array([[ 0.04328007, -0.01541193,  0.07371561, ..., -0.0231898 ,\n          0.06690944,  0.0498458 ],\n        [ 0.06966721, -0.02854891, -0.04922764, ...,  0.020246  ,\n          0.08693338, -0.06307527],\n        [ 0.05585556, -0.13589333,  0.0858058 , ..., -0.03775901,\n          0.06489471,  0.12298056],\n        ...,\n        [ 0.17368092, -0.10051101, -0.07729208, ...,  0.12591116,\n          0.01128803,  0.01038946],\n        [-0.25311124, -0.07464317,  0.12590763, ...,  0.10571449,\n         -0.01390422,  0.01686078],\n        [-0.21493909, -0.08434267,  0.00401744, ..., -0.08825056,\n          0.09488003, -0.18868515]], dtype=float32),\n array([-4.0599257e-02, -5.7264104e-02, -7.4285720e-03, -8.8432794e-03,\n         8.2456782e-02, -6.7039967e-02,  8.0323078e-02, -5.0174162e-02,\n        -5.2275632e-02, -3.0707238e-02,  3.9485977e-03, -3.4528419e-02,\n         3.1738058e-02, -9.7941160e-03, -2.2923967e-02, -3.0179033e-03,\n        -2.4778739e-02, -1.7759549e-02, -1.6397475e-05, -5.5817537e-02,\n        -2.9840395e-02, -5.4178268e-02, -1.5251145e-02, -4.6187498e-02,\n         4.6387676e-02,  1.7519236e-02,  1.6500271e-03, -3.0936431e-02,\n        -3.5580751e-02, -4.5786705e-02, -3.8228117e-02, -1.2649813e-02,\n        -3.1898715e-02, -9.2574887e-02, -3.2500431e-02, -2.8738972e-02,\n         8.0105029e-02,  7.5168334e-02,  8.9080147e-03, -2.2506507e-02,\n        -3.3994464e-03, -4.6606362e-02, -4.3367065e-02, -2.5138935e-02,\n        -1.9473448e-02, -3.8542982e-02,  5.2442826e-02,  5.8593579e-02,\n        -6.3184239e-02,  2.6102478e-02, -5.2004270e-02,  7.4336715e-02,\n         6.7887791e-02,  3.4508009e-03, -2.8900936e-02, -1.0594211e-02,\n        -7.0409328e-02, -2.7203545e-02, -1.9214042e-02, -3.7138134e-02,\n        -3.7783463e-02,  6.9744992e-03, -2.3238702e-02,  4.5755897e-02,\n        -1.4406297e-02,  6.1146587e-02,  2.4701228e-02, -6.6888114e-03,\n         1.1397119e-02, -4.5143735e-02,  4.8228540e-02,  7.9887643e-02,\n         4.9868849e-04,  3.2010511e-02, -5.8974013e-02,  7.3359422e-03,\n        -2.6772553e-03,  1.3173760e-02,  9.4132833e-03,  2.8448399e-02,\n         1.6734790e-02, -2.1803390e-02,  1.5556789e-03,  8.7715574e-03,\n        -1.5456495e-02, -1.9304024e-03, -2.3798410e-02, -2.9597890e-02,\n         2.0707576e-04,  3.8909487e-02,  9.5246028e-04, -1.1717834e-03,\n        -2.9852558e-02, -1.4364694e-02,  2.0199522e-02,  6.6019339e-03,\n         5.9525620e-02,  3.3365022e-02, -1.0951227e-02, -3.1945176e-02,\n        -5.8196811e-03, -8.4137760e-02, -1.5424845e-02, -3.2670423e-04,\n        -6.2907636e-02, -1.4495441e-02, -1.6684869e-02, -6.2152900e-02,\n        -2.4261808e-02,  3.2355636e-02,  7.0157818e-02,  2.0310606e-03,\n         5.1574744e-03, -5.4675985e-02, -1.1066000e-03, -4.1320026e-02,\n        -3.7530325e-02, -8.4429048e-02, -1.6617790e-02,  3.0863607e-02,\n        -2.5835192e-02,  3.1204117e-02, -5.4024715e-02,  5.1892857e-04,\n        -3.6859579e-02,  2.9598547e-02, -1.1735910e-02,  1.0282493e-02],\n       dtype=float32),\n array([[ 0.14389628],\n        [-0.02603454],\n        [-0.3323112 ],\n        [-0.0223282 ],\n        [-0.14150445],\n        [ 0.08694505],\n        [ 0.09101626],\n        [-0.1865764 ],\n        [-0.00577949],\n        [ 0.12538813],\n        [ 0.03488667],\n        [ 0.15918285],\n        [-0.10256492],\n        [ 0.34697112],\n        [ 0.22413033],\n        [ 0.14429918],\n        [ 0.09092712],\n        [ 0.07078008],\n        [ 0.05698001],\n        [ 0.35658297],\n        [ 0.00392762],\n        [ 0.05513742],\n        [ 0.00756801],\n        [ 0.11970688],\n        [ 0.14928044],\n        [ 0.01716309],\n        [ 0.30566293],\n        [-0.04030254],\n        [-0.08040912],\n        [ 0.0573553 ],\n        [-0.0866798 ],\n        [ 0.15025033],\n        [ 0.06525314],\n        [ 0.01146421],\n        [ 0.02980965],\n        [ 0.07444808],\n        [ 0.03575139],\n        [ 0.08587829],\n        [ 0.05885594],\n        [ 0.25206035],\n        [ 0.18433544],\n        [ 0.07271486],\n        [ 0.17745449],\n        [-0.06006883],\n        [ 0.01526834],\n        [-0.01771391],\n        [ 0.07014848],\n        [-0.74401623],\n        [-0.06567132],\n        [ 0.19546822],\n        [ 0.08150794],\n        [ 0.04136789],\n        [-0.02718604],\n        [-0.02843481],\n        [-0.00627032],\n        [-0.24783649],\n        [ 0.04651732],\n        [-0.03742632],\n        [ 0.13896212],\n        [ 0.02563263],\n        [-0.30408585],\n        [ 0.00953491],\n        [-0.21869174],\n        [-0.10209536],\n        [ 0.1568792 ],\n        [-0.02467103],\n        [-0.15456489],\n        [-0.08183491],\n        [ 0.10214016],\n        [ 0.15950505],\n        [ 0.07797687],\n        [ 0.10623689],\n        [ 0.16664886],\n        [ 0.12989655],\n        [ 0.2241273 ],\n        [ 0.17192532],\n        [ 0.08766229],\n        [-0.06161723],\n        [ 0.3677314 ],\n        [ 0.02401656],\n        [-0.21219066],\n        [ 0.02798242],\n        [ 0.0695188 ],\n        [ 0.22989808],\n        [ 0.06167556],\n        [-0.01385407],\n        [-0.0502211 ],\n        [-0.20300207],\n        [-0.01362527],\n        [-0.03842722],\n        [-0.26266715],\n        [-0.19200699],\n        [ 0.02920733],\n        [-0.00885551],\n        [ 0.1072671 ],\n        [-0.20765781],\n        [-0.10337486],\n        [ 0.1335081 ],\n        [-0.17483425],\n        [-0.00856401],\n        [ 0.22114882],\n        [ 0.13440225],\n        [ 0.02169876],\n        [-0.16414063],\n        [-0.23402935],\n        [ 0.13558051],\n        [-0.07902306],\n        [ 0.06798872],\n        [ 0.00820846],\n        [-0.12688951],\n        [ 0.0866529 ],\n        [-0.00927667],\n        [ 0.04001361],\n        [-0.00285994],\n        [ 0.14018391],\n        [ 0.18113603],\n        [-0.10597079],\n        [-0.04832596],\n        [ 0.24848294],\n        [-0.00634585],\n        [ 0.15186846],\n        [ 0.26860097],\n        [ 0.1243847 ],\n        [ 0.11618798],\n        [-0.01583515],\n        [ 0.15891112],\n        [ 0.20059505],\n        [-0.17265472]], dtype=float32),\n array([1.1110713], dtype=float32)]"},"metadata":{}}]},{"cell_type":"code","source":"# Step 2: Helper Functions\n# Function to decode reviews\ndef decode_review(encoded_review):\n    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n\n# Function to preprocess user input\ndef preprocess_text(text):\n    words = text.lower().split()\n    encoded_review = [word_index.get(word, 2) + 3 for word in words]\n    padded_review = sequence.pad_sequences([encoded_review], maxlen=500)\n    return padded_review","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:57.638240Z","iopub.execute_input":"2024-08-23T12:18:57.638610Z","iopub.status.idle":"2024-08-23T12:18:57.644512Z","shell.execute_reply.started":"2024-08-23T12:18:57.638568Z","shell.execute_reply":"2024-08-23T12:18:57.643606Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### 1. **Helper Functions:**\nThe code defines two helper functions: `decode_review` and `preprocess_text`. These functions help in handling text data, specifically movie reviews, by converting them into a form that can be used by a machine learning model.\n\n### 2. **`decode_review(encoded_review)` Function:**\n- **Purpose:** This function takes an encoded review (a list of numbers) and converts it back into a readable sentence (a string of words).\n- **How It Works:** \n  - It loops through each number in the encoded review.\n  - For each number, it looks up the corresponding word using `reverse_word_index`.\n  - The number is reduced by 3 (`i - 3`) because in the encoding, some numbers are reserved for special tokens.\n  - If the word isn't found, it uses a question mark `'?'` as a placeholder.\n  - Finally, it joins all the words together into a single string with spaces in between.\n\n### 3. **`preprocess_text(text)` Function:**\n- **Purpose:** This function prepares a user's text input (like a movie review) for the machine learning model.\n- **How It Works:**\n  - It converts the text to lowercase to ensure consistency.\n  - The text is split into individual words.\n  - Each word is looked up in `word_index`, which is a dictionary mapping words to numbers. If a word isn't found, it's given a default value of `2`.\n  - Each number is increased by 3, again reserving the first few numbers for special tokens.\n  - The resulting list of numbers (encoded review) is then padded using `sequence.pad_sequences` to ensure it's exactly 500 numbers long (adding zeros if necessary).\n  - The padded review is returned, ready for input into the model.\n\n### Summary:\n- `decode_review` translates numbers back into human-readable words.\n- `preprocess_text` converts a user's text input into a numerical format that the machine learning model can understand, ensuring it's the right length.","metadata":{}},{"cell_type":"code","source":"preprocessed_input = preprocess_text(\"This movie was fantastic! The acting was great and the plot was thrilling.\")\nprediction = model.predict(preprocessed_input)\nsentiment = 'Positive' if prediction[0][0] > 0.5 else 'Negative'\nprint(sentiment)\nprint(prediction[0][0])","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:57.645660Z","iopub.execute_input":"2024-08-23T12:18:57.646060Z","iopub.status.idle":"2024-08-23T12:18:57.962114Z","shell.execute_reply.started":"2024-08-23T12:18:57.645994Z","shell.execute_reply":"2024-08-23T12:18:57.961207Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\nPositive\n0.6535551\n","output_type":"stream"}]},{"cell_type":"code","source":"### Prediction  function\n\ndef predict_sentiment(review):\n    preprocessed_input=preprocess_text(review)\n    prediction=model.predict(preprocessed_input)\n    sentiment = 'Positive' if prediction[0][0] > 0.5 else 'Negative'\n    return sentiment, prediction[0][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:58.042682Z","iopub.execute_input":"2024-08-23T12:18:58.043062Z","iopub.status.idle":"2024-08-23T12:18:58.050578Z","shell.execute_reply.started":"2024-08-23T12:18:58.043021Z","shell.execute_reply":"2024-08-23T12:18:58.048905Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Step 4: User Input and Prediction\n# Example review for prediction\nexample_review = \"This movie was fantastic! The acting was great and the plot was thrilling.\"\n\nsentiment,score=predict_sentiment(example_review)\n\nprint(f'Review: {example_review}')\nprint(f'Sentiment: {sentiment}')\nprint(f'Prediction Score: {score}')","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:58.051832Z","iopub.execute_input":"2024-08-23T12:18:58.052228Z","iopub.status.idle":"2024-08-23T12:18:58.128592Z","shell.execute_reply.started":"2024-08-23T12:18:58.052184Z","shell.execute_reply":"2024-08-23T12:18:58.127762Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\nReview: This movie was fantastic! The acting was great and the plot was thrilling.\nSentiment: Positive\nPrediction Score: 0.6535550951957703\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 4: User Input and Prediction\n# Example review for prediction\nexample_review = \"This movie was great! The acting was great and the plot was thrilling.\"\n\nsentiment,score=predict_sentiment(example_review)\n\nprint(f'Review: {example_review}')\nprint(f'Sentiment: {sentiment}')\nprint(f'Prediction Score: {score}')","metadata":{"execution":{"iopub.status.busy":"2024-08-23T12:18:58.129766Z","iopub.execute_input":"2024-08-23T12:18:58.130223Z","iopub.status.idle":"2024-08-23T12:18:58.192545Z","shell.execute_reply.started":"2024-08-23T12:18:58.130179Z","shell.execute_reply":"2024-08-23T12:18:58.191713Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nReview: This movie was great! The acting was great and the plot was thrilling.\nSentiment: Positive\nPrediction Score: 0.6535550951957703\n","output_type":"stream"}]}]}