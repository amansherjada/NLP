{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T20:48:24.389794Z","iopub.execute_input":"2024-07-19T20:48:24.390195Z","iopub.status.idle":"2024-07-19T20:48:25.554299Z","shell.execute_reply.started":"2024-07-19T20:48:24.390161Z","shell.execute_reply":"2024-07-19T20:48:25.552975Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Preprocessing\n\n**Text preprocessing in NLP** is the process of cleaning and preparing raw text data for analysis. It includes steps like removing noise (e.g., HTML tags, special characters), normalizing case, correcting spelling errors, tokenizing text into words or sentences, removing stop words, stripping punctuation, and performing lemmatization or stemming to reduce words to their base forms. Advanced preprocessing may involve POS tagging, named entity recognition, and feature extraction techniques such as TF-IDF or word embeddings. This process enhances the quality and performance of NLP models.","metadata":{}},{"cell_type":"markdown","source":"## Lowercasing\n\n**Lowercasing** refers to the process of converting all characters in a text to lowercase. This standardization helps in reducing the complexity of text data by treating words with different cases (e.g., \"Apple\" and \"apple\") as the same word, thereby improving the efficiency and accuracy of subsequent text processing and analysis steps. Lowercasing is particularly useful in ensuring uniformity and consistency in the dataset.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:25.556278Z","iopub.execute_input":"2024-07-19T20:48:25.557147Z","iopub.status.idle":"2024-07-19T20:48:27.038219Z","shell.execute_reply.started":"2024-07-19T20:48:25.557114Z","shell.execute_reply":"2024-07-19T20:48:27.037049Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.039443Z","iopub.execute_input":"2024-07-19T20:48:27.039843Z","iopub.status.idle":"2024-07-19T20:48:27.060597Z","shell.execute_reply.started":"2024-07-19T20:48:27.039804Z","shell.execute_reply":"2024-07-19T20:48:27.059317Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df[\"review\"][5].lower()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.063601Z","iopub.execute_input":"2024-07-19T20:48:27.064107Z","iopub.status.idle":"2024-07-19T20:48:27.071858Z","shell.execute_reply.started":"2024-07-19T20:48:27.064068Z","shell.execute_reply":"2024-07-19T20:48:27.070764Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times in the last 25 years. paul lukas\\' performance brings tears to my eyes, and bette davis, in one of her very few truly sympathetic roles, is a delight. the kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. and the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. if i had a dozen thumbs, they\\'d all be \"up\" for this movie.'"},"metadata":{}}]},{"cell_type":"code","source":"df[\"review\"] = df[\"review\"].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.073312Z","iopub.execute_input":"2024-07-19T20:48:27.074392Z","iopub.status.idle":"2024-07-19T20:48:27.305202Z","shell.execute_reply.started":"2024-07-19T20:48:27.074350Z","shell.execute_reply":"2024-07-19T20:48:27.303966Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df[\"review\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.306458Z","iopub.execute_input":"2024-07-19T20:48:27.306838Z","iopub.status.idle":"2024-07-19T20:48:27.315762Z","shell.execute_reply.started":"2024-07-19T20:48:27.306802Z","shell.execute_reply":"2024-07-19T20:48:27.314705Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0        one of the other reviewers has mentioned that ...\n1        a wonderful little production. <br /><br />the...\n2        i thought this was a wonderful way to spend ti...\n3        basically there's a family where a little boy ...\n4        petter mattei's \"love in the time of money\" is...\n                               ...                        \n49995    i thought this movie did a down right good job...\n49996    bad plot, bad dialogue, bad acting, idiotic di...\n49997    i am a catholic taught in parochial elementary...\n49998    i'm going to have to disagree with the previou...\n49999    no one expects the star trek movies to be high...\nName: review, Length: 50000, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"## Remove HTML tags using Regular expressions\n\nWe remove HTML tags from text for several key reasons:\n\n1. **Clean Text**: HTML tags don't contribute to the actual content, only to its structure and presentation.\n2. **Normalization**: Removing tags helps standardize the text, making it easier to process uniformly.\n3. **Preprocessing**: Tags can interfere with tokenization and other text processing steps.\n4. **Accuracy**: Clean text improves the performance of NLP models by focusing on meaningful content.\n5. **Consistency**: Ensures uniformity across different text sources, simplifying downstream tasks.","metadata":{}},{"cell_type":"code","source":"import re\ndef remove_html_tags(text):\n    pattern = re.compile(\"<.*?>\")\n    return pattern.sub(r\"\", text)    ","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.317104Z","iopub.execute_input":"2024-07-19T20:48:27.317459Z","iopub.status.idle":"2024-07-19T20:48:27.324875Z","shell.execute_reply.started":"2024-07-19T20:48:27.317432Z","shell.execute_reply":"2024-07-19T20:48:27.323562Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"text = \"<html><body><p> File </p><p> Author - Aman Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.326162Z","iopub.execute_input":"2024-07-19T20:48:27.326541Z","iopub.status.idle":"2024-07-19T20:48:27.336731Z","shell.execute_reply.started":"2024-07-19T20:48:27.326475Z","shell.execute_reply":"2024-07-19T20:48:27.335446Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"remove_html_tags(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.338847Z","iopub.execute_input":"2024-07-19T20:48:27.339305Z","iopub.status.idle":"2024-07-19T20:48:27.350811Z","shell.execute_reply.started":"2024-07-19T20:48:27.339262Z","shell.execute_reply":"2024-07-19T20:48:27.349605Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"' File  Author - Aman Khan Click here to download'"},"metadata":{}}]},{"cell_type":"code","source":"df['review'] = df['review'].apply(remove_html_tags)\ndf['review'][7]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.355356Z","iopub.execute_input":"2024-07-19T20:48:27.356310Z","iopub.status.idle":"2024-07-19T20:48:27.599152Z","shell.execute_reply.started":"2024-07-19T20:48:27.356267Z","shell.execute_reply":"2024-07-19T20:48:27.597729Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"this show was an amazing, fresh & innovative idea in the 70's when it first aired. the first 7 or 8 years were brilliant, but things dropped off after that. by 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.it's truly disgraceful how far this show has fallen. the writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. i find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. how can one recognize such brilliance and then see fit to replace it with such mediocrity? i felt i must give 2 stars out of respect for the original cast that made this show such a huge success. as it is now, the show is just awful. i can't believe it's still on the air.\""},"metadata":{}}]},{"cell_type":"markdown","source":"## Removing URLs\n\nIn NLP, removing URLs from text is important for several reasons:\n\n1. **Noise Reduction**: URLs are often irrelevant to the text's main content and can introduce noise, affecting the quality of text analysis.\n\n2. **Normalization**: Like HTML tags, URLs can disrupt the uniform processing of text, complicating tokenization and other preprocessing steps.\n\n3. **Improved Model Performance**: Clean text without URLs helps NLP models focus on meaningful content, leading to better performance.\n\n4. **Consistency**: Removing URLs ensures a consistent text format across different sources, simplifying text processing and analysis.\n\n5. **Privacy and Security**: URLs can contain sensitive information or lead to security risks, so removing them helps in maintaining privacy and security.\n\nOverall, removing URLs is a standard preprocessing step to ensure cleaner, more consistent, and useful text for NLP tasks.","metadata":{}},{"cell_type":"code","source":"def remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.604929Z","iopub.execute_input":"2024-07-19T20:48:27.607961Z","iopub.status.idle":"2024-07-19T20:48:27.616585Z","shell.execute_reply.started":"2024-07-19T20:48:27.607904Z","shell.execute_reply":"2024-07-19T20:48:27.615041Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"This Python function `remove_url` is designed to remove URLs from a given text string.\n\n**Regular Expression Compilation**:\n   ```python\n   pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n   ```\n   This line compiles a regular expression (regex) pattern into a regex object for later use. The pattern `r'https?://\\S+|www\\.\\S+'` is used to match URLs:\n   - `https?://`: Matches `http://` or `https://`. The `s?` part makes the `s` optional, so it matches both `http` and `https`.\n   - `\\S+`: Matches one or more non-whitespace characters, effectively capturing the entire URL.\n   - `|`: Acts as an OR operator, meaning the pattern will match either the left side (`https?://\\S+`) or the right side (`www\\.\\S+`).\n   - `www\\.\\S+`: Matches URLs starting with `www.` followed by one or more non-whitespace characters.\n","metadata":{}},{"cell_type":"code","source":"text1 = 'Check out my Facecook https://www.facebook.com/'\ntext2 = 'Check out my Instagram https://www.instagram.com/'\ntext3 = 'Google search here www.google.com'\ntext4 = 'For GitHub click https://github.com/ to search check www.google.com'","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.618175Z","iopub.execute_input":"2024-07-19T20:48:27.618884Z","iopub.status.idle":"2024-07-19T20:48:27.627333Z","shell.execute_reply.started":"2024-07-19T20:48:27.618846Z","shell.execute_reply":"2024-07-19T20:48:27.626164Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"remove_url(text2)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.628879Z","iopub.execute_input":"2024-07-19T20:48:27.629562Z","iopub.status.idle":"2024-07-19T20:48:27.642050Z","shell.execute_reply.started":"2024-07-19T20:48:27.629521Z","shell.execute_reply":"2024-07-19T20:48:27.640891Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'Check out my Instagram '"},"metadata":{}}]},{"cell_type":"markdown","source":"## Removing punctuation (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n\nIn NLP, removing punctuation helps:\n\n1. **Simplify Text**: Reduces complexity for processing.\n2. **Normalize Data**: Ensures uniform text format.\n3. **Improve Tokenization**: Prevents punctuation from affecting word splits.\n4. **Enhance Model Performance**: Focuses on meaningful content for better results.\n5. **Size**: Punctuation makes the document large.","metadata":{}},{"cell_type":"code","source":"import string, time\nstring.punctuation","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.647058Z","iopub.execute_input":"2024-07-19T20:48:27.647838Z","iopub.status.idle":"2024-07-19T20:48:27.662407Z","shell.execute_reply.started":"2024-07-19T20:48:27.647794Z","shell.execute_reply":"2024-07-19T20:48:27.660942Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"},"metadata":{}}]},{"cell_type":"code","source":"exclude = string.punctuation\nexclude","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.664187Z","iopub.execute_input":"2024-07-19T20:48:27.664939Z","iopub.status.idle":"2024-07-19T20:48:27.677168Z","shell.execute_reply.started":"2024-07-19T20:48:27.664894Z","shell.execute_reply":"2024-07-19T20:48:27.675903Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"},"metadata":{}}]},{"cell_type":"code","source":"def remove_punctuation(text):\n    for char in exclude:\n        text = text.replace(char, \"\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.679127Z","iopub.execute_input":"2024-07-19T20:48:27.679815Z","iopub.status.idle":"2024-07-19T20:48:27.684972Z","shell.execute_reply.started":"2024-07-19T20:48:27.679773Z","shell.execute_reply":"2024-07-19T20:48:27.683890Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"text = \"Hello, world! This is a test: do you like it? Yes, I do... A lot; really! How about you? @username #hashtag $dollar %percent ^caret &amp *star (parentheses) -dash_underscore+plus=equals{curly}brackets[brackets]|\\backslash~tilde`backtick\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.686559Z","iopub.execute_input":"2024-07-19T20:48:27.687174Z","iopub.status.idle":"2024-07-19T20:48:27.699173Z","shell.execute_reply.started":"2024-07-19T20:48:27.687137Z","shell.execute_reply":"2024-07-19T20:48:27.697889Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"remove_punctuation(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.700772Z","iopub.execute_input":"2024-07-19T20:48:27.701574Z","iopub.status.idle":"2024-07-19T20:48:27.711429Z","shell.execute_reply.started":"2024-07-19T20:48:27.701531Z","shell.execute_reply":"2024-07-19T20:48:27.710240Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'Hello world This is a test do you like it Yes I do A lot really How about you username hashtag dollar percent caret amp star parentheses dashunderscoreplusequalscurlybracketsbrackets\\x08ackslashtildebacktick'"},"metadata":{}}]},{"cell_type":"code","source":"start=time.time()\nprint(remove_punctuation(text))\ntime1=time.time()-start\nprint(time1)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.713603Z","iopub.execute_input":"2024-07-19T20:48:27.713997Z","iopub.status.idle":"2024-07-19T20:48:27.720583Z","shell.execute_reply.started":"2024-07-19T20:48:27.713960Z","shell.execute_reply":"2024-07-19T20:48:27.719390Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Hello world This is a test do you like it Yes I do A lot really How about you username hashtag dollar percent caret amp star parentheses dashunderscoreplusequalscurlybracketsbracketackslashtildebacktick\n0.0001456737518310547\n","output_type":"stream"}]},{"cell_type":"code","source":"def remove_punctuation2(text):\n    return text.translate(str.maketrans('','',exclude))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.722473Z","iopub.execute_input":"2024-07-19T20:48:27.722898Z","iopub.status.idle":"2024-07-19T20:48:27.730397Z","shell.execute_reply.started":"2024-07-19T20:48:27.722860Z","shell.execute_reply":"2024-07-19T20:48:27.729364Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"start=time.time()\nprint(remove_punctuation2(text))\ntime2=time.time()-start\nprint(time2)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.731528Z","iopub.execute_input":"2024-07-19T20:48:27.731862Z","iopub.status.idle":"2024-07-19T20:48:27.743419Z","shell.execute_reply.started":"2024-07-19T20:48:27.731834Z","shell.execute_reply":"2024-07-19T20:48:27.742295Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Hello world This is a test do you like it Yes I do A lot really How about you username hashtag dollar percent caret amp star parentheses dashunderscoreplusequalscurlybracketsbracketackslashtildebacktick\n0.0002143383026123047\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Common chat abbreviations and slang\n\nHandling chat words in NLP is crucial for several reasons:\n\n1. **Improved Understanding**: Expanding chat abbreviations helps models better understand the content.\n2. **Contextual Accuracy**: Many chat words affect sentiment, tone, or intent (e.g., \"LOL\" vs. \"Laughing Out Loud\").\n3. **Data Normalization**: Ensures uniformity and consistency in text data, simplifying processing and analysis.\n4. **Enhanced Model Training**: Models trained on expanded forms of chat words perform more accurately.\n5. **Sentiment Analysis**: Properly handling chat words ensures more accurate sentiment detection (e.g., \"LMAO\" indicates strong amusement).\n6. **Readability**: Expanded chat words are clearer for both humans and NLP tasks like summarization or translation.","metadata":{}},{"cell_type":"code","source":"chat_word = {\n    'AFAIK': 'As Far As I Know',\n    'AFK': 'Away From Keyboard',\n    'ASAP': 'As Soon As Possible',\n    'ATK': 'At The Keyboard',\n    'ATM': 'At The Moment',\n    'A3': 'Anytime, Anywhere, Anyplace',\n    'BAK': 'Back At Keyboard',\n    'BBL': 'Be Back Later',\n    'BBS': 'Be Back Soon',\n    'BFN': 'Bye For Now',\n    'B4N': 'Bye For Now',\n    'BRB': 'Be Right Back',\n    'BRT': 'Be Right There',\n    'BTW': 'By The Way',\n    'B4': 'Before',\n    'CU': 'See You',\n    'CUL8R': 'See You Later',\n    'CYA': 'See You',\n    'FAQ': 'Frequently Asked Questions',\n    'FC': 'Fingers Crossed',\n    'FWIW': \"For What It's Worth\",\n    'FYI': 'For Your Information',\n    'GAL': 'Get A Life',\n    'GG': 'Good Game',\n    'GN': 'Good Night',\n    'GMTA': 'Great Minds Think Alike',\n    'GR8': 'Great!',\n    'G9': 'Genius',\n    'IC': 'I See',\n    'ICQ': 'I Seek you (also a chat program)',\n    'ILU': 'ILU: I Love You',\n    'IMHO': 'In My Honest/Humble Opinion',\n    'IMO': 'In My Opinion',\n    'IOW': 'In Other Words',\n    'IRL': 'In Real Life',\n    'KISS': 'Keep It Simple, Stupid',\n    'LDR': 'Long Distance Relationship',\n    'LMAO': 'Laugh My A.. Off',\n    'LOL': 'Laughing Out Loud',\n    'LTNS': 'Long Time No See',\n    'L8R': 'Later',\n    'MTE': 'My Thoughts Exactly',\n    'M8': 'Mate',\n    'NRN': 'No Reply Necessary',\n    'OIC': 'Oh I See',\n    'PITA': 'Pain In The A..',\n    'PRT': 'Party',\n    'PRW': 'Parents Are Watching',\n    'QPSA?': 'Que Pasa?',\n    'ROFL': 'Rolling On The Floor Laughing',\n    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n    'SK8': 'Skate',\n    'STATS': 'Your sex and age',\n    'ASL': 'Age, Sex, Location',\n    'THX': 'Thank You',\n    'TTFN': 'Ta-Ta For Now!',\n    'TTYL': 'Talk To You Later',\n    'U': 'You',\n    'U2': 'You Too',\n    'U4E': 'Yours For Ever',\n    'WB': 'Welcome Back',\n    'WTF': 'What The F...',\n    'WTG': 'Way To Go!',\n    'WUF': 'Where Are You From?',\n    'W8': 'Wait...',\n    '7K': 'Sick:-D Laugher',\n    'TFW': 'That feeling when',\n    'MFW': 'My face when',\n    'MRW': 'My reaction when',\n    'IFYP': 'I feel your pain',\n    'TNTL': 'Trying not to laugh',\n    'JK': 'Just kidding',\n    'IDC': \"I don't care\",\n    'ILY': 'I love you',\n    'IMU': 'I miss you',\n    'ADIH': 'Another day in hell',\n    'ZZZ': 'Sleeping, bored, tired',\n    'WYWH': 'Wish you were here',\n    'TIME': 'Tears in my eyes',\n    'BAE': 'Before anyone else',\n    'FIMH': 'Forever in my heart',\n    'BSAAW': 'Big smile and a wink',\n    'BWL': 'Bursting with laughter',\n    'BFF': 'Best friends forever',\n    'CSL': \"Can't stop laughing\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.745261Z","iopub.execute_input":"2024-07-19T20:48:27.745633Z","iopub.status.idle":"2024-07-19T20:48:27.756827Z","shell.execute_reply.started":"2024-07-19T20:48:27.745604Z","shell.execute_reply":"2024-07-19T20:48:27.755650Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def short_conv(text):\n    new_text = []  # Initialize an empty list to hold the processed words\n    for w in text.split():  # Split the input text into words and iterate over them\n        if w.upper() in chat_word:  # Check if the uppercase version of the word is in the chat_word dictionary\n            new_text.append(chat_word[w.upper()])  # If it is, append the full form from the dictionary to new_text\n        else:\n            new_text.append(w)  # If it is not, append the original word to new_text\n    return \" \".join(new_text)  # Join the processed words into a single string and return it","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.758293Z","iopub.execute_input":"2024-07-19T20:48:27.758730Z","iopub.status.idle":"2024-07-19T20:48:27.773325Z","shell.execute_reply.started":"2024-07-19T20:48:27.758694Z","shell.execute_reply":"2024-07-19T20:48:27.772308Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"short_conv(\"LOL I will BRB\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.774982Z","iopub.execute_input":"2024-07-19T20:48:27.775390Z","iopub.status.idle":"2024-07-19T20:48:27.786728Z","shell.execute_reply.started":"2024-07-19T20:48:27.775352Z","shell.execute_reply":"2024-07-19T20:48:27.785562Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'Laughing Out Loud I will Be Right Back'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Spelling correction\n\nSpelling correction in NLP is done to improve text quality and ensure accurate analysis. Correcting spelling errors helps in:\n\n1. **Enhanced Understanding**: Ensures that words are recognized correctly by NLP models.\n2. **Data Consistency**: Maintains uniformity in text data.\n3. **Improved Model Performance**: Reduces noise, leading to better model training and predictions.\n4. **Accurate Results**: Improves the accuracy of tasks like sentiment analysis, information retrieval, and machine translation.","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nincorrect_text = \"Ths is an exmple of a sentnce with sevral speling erors.\"\n\ntextblb = TextBlob(incorrect_text)\ntextblb.correct().string","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:27.788013Z","iopub.execute_input":"2024-07-19T20:48:27.788420Z","iopub.status.idle":"2024-07-19T20:48:29.997466Z","shell.execute_reply.started":"2024-07-19T20:48:27.788391Z","shell.execute_reply":"2024-07-19T20:48:29.996161Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'The is an example of a sentence with several spelling errors.'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Removing StopWords\n\nRemoving stop words in NLP text processing is like cleaning up unnecessary words like \"the\", \"is\", and \"and\" from sentences. These words appear frequently in language but don't add much meaning. By getting rid of them, we focus more on the important words that carry the actual message, making our analysis faster and more accurate. It's like decluttering a room so you can see and understand the important things better.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstopwords.words(\"english\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:29.999198Z","iopub.execute_input":"2024-07-19T20:48:30.000030Z","iopub.status.idle":"2024-07-19T20:48:30.017986Z","shell.execute_reply.started":"2024-07-19T20:48:29.999987Z","shell.execute_reply":"2024-07-19T20:48:30.016517Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"},"metadata":{}}]},{"cell_type":"code","source":"def remove_stopwords(text):\n    new_text=[]\n    for word in text.split():\n        if word in stopwords.words('english'):\n            new_text.append('')\n        else:\n            new_text.append(word)\n            \n    x = new_text[:]  # Create a copy of new_text\n    new_text.clear()  # Clear the original new_text list\n    return \" \".join(x)  # Join the copied list x into a single string separated by spaces and return it\n\ntext = \"The quick brown fox jumps over the lazy dog. In a nutshell, it's all about how you can improve your writing skills by using the right words in the right context.\"\nremove_stopwords(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.019378Z","iopub.execute_input":"2024-07-19T20:48:30.019850Z","iopub.status.idle":"2024-07-19T20:48:30.043213Z","shell.execute_reply.started":"2024-07-19T20:48:30.019818Z","shell.execute_reply":"2024-07-19T20:48:30.042112Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'The quick brown fox jumps   lazy dog. In  nutshell,       improve  writing skills  using  right words   right context.'"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.051766Z","iopub.execute_input":"2024-07-19T20:48:30.052166Z","iopub.status.idle":"2024-07-19T20:48:30.063132Z","shell.execute_reply.started":"2024-07-19T20:48:30.052134Z","shell.execute_reply":"2024-07-19T20:48:30.061886Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  one of the other reviewers has mentioned that ...  positive\n1  a wonderful little production. the filming tec...  positive\n2  i thought this was a wonderful way to spend ti...  positive\n3  basically there's a family where a little boy ...  negative\n4  petter mattei's \"love in the time of money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a wonderful little production. the filming tec...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei's \"love in the time of money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df['review'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.065072Z","iopub.execute_input":"2024-07-19T20:48:30.065961Z","iopub.status.idle":"2024-07-19T20:48:30.074631Z","shell.execute_reply.started":"2024-07-19T20:48:30.065919Z","shell.execute_reply":"2024-07-19T20:48:30.073387Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Handling emojis\n\nHandling emojis in NLP text processing is important because emojis convey emotional and contextual information that traditional text alone may not fully capture. Here's why it matters:\n\n1. **Emotional Context**: Emojis provide emotional cues such as happiness ðŸ˜Š, sadness ðŸ˜¢, or surprise ðŸ˜®, which are crucial for sentiment analysis and understanding the tone of text.\n\n2. **Enhanced Meaning**: They enrich the meaning of text by adding nuances that words alone might not express effectively. For example, \"I'm excited!\" might convey more with a ðŸ˜ƒ emoji.\n\n3. **Communication Style**: Emojis reflect modern communication styles and can impact how messages are interpreted in social media, customer feedback, or online reviews.\n\n4. **Choice of Handling**: Depending on the application, emojis can be removed to focus purely on textual analysis, or they can be replaced with their textual description (emojis like ðŸ˜Š become \"smiling face with smiling eyes\").","metadata":{}},{"cell_type":"code","source":"# Removing Emojis\ndef remove_emoji(text):\n    emoji_pattern=re.compile(\"[\"\n                             u\"\\U0001F600-\\U0001F64F\" #emoticons\n                             u\"\\U0001F300-\\U0001F5FF\" #symbols, pictograph\n                              u\"\\U0001F680-\\U0001F6FF\" #transport and map symbol\n                              u\"\\U0001F1E0-\\U0001F1FF\" #flags(IOS)\n                              u\"\\U00002702-\\U000027B0\"\n                              u\"\\U00002FC2-\\U0001F251\"\n                             \"]+\",flags=re.UNICODE)\n    return emoji_pattern.sub(r'',text)\n\ntext=\"I'm so excited for the party tonight! ðŸŽ‰ Can't wait to see everyone there! ðŸ˜„\"\nremove_emoji(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.076344Z","iopub.execute_input":"2024-07-19T20:48:30.076809Z","iopub.status.idle":"2024-07-19T20:48:30.098843Z","shell.execute_reply.started":"2024-07-19T20:48:30.076772Z","shell.execute_reply":"2024-07-19T20:48:30.097598Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"\"I'm so excited for the party tonight!  Can't wait to see everyone there! \""},"metadata":{}}]},{"cell_type":"code","source":"#Replacing Emojis\nimport emoji\nprint(emoji.demojize(text))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.100356Z","iopub.execute_input":"2024-07-19T20:48:30.100814Z","iopub.status.idle":"2024-07-19T20:48:30.170031Z","shell.execute_reply.started":"2024-07-19T20:48:30.100776Z","shell.execute_reply":"2024-07-19T20:48:30.168628Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"I'm so excited for the party tonight! :party_popper: Can't wait to see everyone there! :grinning_face_with_smiling_eyes:\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenization\n\n#### What is Tokenization?\n\nTokenization is the process of breaking down text into smaller pieces called tokens. These tokens can be words, phrases, or even individual characters, depending on the application. Think of it like cutting a paragraph into smaller, manageable parts.\n\n#### Example:\n\nImagine you have this sentence:\n\n```plaintext\nI love eating pizza!\n```\n\nWhen we tokenize it into words, it becomes:\n\n```plaintext\n[\"I\", \"love\", \"eating\", \"pizza\", \"!\"]\n```\n\nEach word and punctuation mark becomes a separate token.\n\n#### Why Do We Use Tokenization in NLP?\n\n1. **Easier Analysis**: Breaking text into tokens makes it easier to analyze. It's like reading a book one word at a time instead of trying to understand it all at once.\n   \n2. **Understanding Context**: It helps in understanding the context of each word in a sentence. For example, knowing that \"love\" is followed by \"eating\" gives a clear picture of the meaning.\n\n3. **Efficient Processing**: Computers can process and analyze tokens more efficiently than long strings of text. It speeds up tasks like searching for specific words or understanding the structure of sentences.\n\n4. **Building Blocks for NLP Tasks**: Tokenization is the first step for many NLP tasks like sentiment analysis, translation, and text summarization. It prepares the text for more complex processing.\n\nTokenization helps break down text into smaller, understandable parts, making it easier for computers to analyze and work with.","metadata":{}},{"cell_type":"markdown","source":"### 1. Split function","metadata":{}},{"cell_type":"code","source":"# word tokenization\nsent1 = 'I am from mumbai'\nsent1.split()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.171526Z","iopub.execute_input":"2024-07-19T20:48:30.173656Z","iopub.status.idle":"2024-07-19T20:48:30.182227Z","shell.execute_reply.started":"2024-07-19T20:48:30.173611Z","shell.execute_reply":"2024-07-19T20:48:30.180954Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"['I', 'am', 'from', 'mumbai']"},"metadata":{}}]},{"cell_type":"code","source":"# sentence tokenization\nsent2 = 'I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\nsent2.split('.')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.184031Z","iopub.execute_input":"2024-07-19T20:48:30.184434Z","iopub.status.idle":"2024-07-19T20:48:30.194649Z","shell.execute_reply.started":"2024-07-19T20:48:30.184393Z","shell.execute_reply":"2024-07-19T20:48:30.193371Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"['I am going to delhi',\n ' I will stay there for 3 days',\n \" Let's hope the trip to be great\"]"},"metadata":{}}]},{"cell_type":"code","source":"# Problems with split function\nsent3 = 'I am going to delhi!!!!'\nsent3.split()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.196309Z","iopub.execute_input":"2024-07-19T20:48:30.196669Z","iopub.status.idle":"2024-07-19T20:48:30.205235Z","shell.execute_reply.started":"2024-07-19T20:48:30.196639Z","shell.execute_reply":"2024-07-19T20:48:30.204031Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"['I', 'am', 'going', 'to', 'delhi!!!!']"},"metadata":{}}]},{"cell_type":"code","source":"# Problems with split function\nsent4 = 'Where do think I should go? I have 3 day holiday'\nsent4.split('.')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.206915Z","iopub.execute_input":"2024-07-19T20:48:30.207346Z","iopub.status.idle":"2024-07-19T20:48:30.216551Z","shell.execute_reply.started":"2024-07-19T20:48:30.207306Z","shell.execute_reply":"2024-07-19T20:48:30.215232Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"['Where do think I should go? I have 3 day holiday']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2. Regular Expression","metadata":{}},{"cell_type":"code","source":"import re\nsent3 = 'I am going to delhi!'\ntokens = re.findall(\"[\\w']+\", sent3)\ntokens","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.218668Z","iopub.execute_input":"2024-07-19T20:48:30.219118Z","iopub.status.idle":"2024-07-19T20:48:30.235917Z","shell.execute_reply.started":"2024-07-19T20:48:30.219078Z","shell.execute_reply":"2024-07-19T20:48:30.234683Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['I', 'am', 'going', 'to', 'delhi']"},"metadata":{}}]},{"cell_type":"code","source":"text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\nsentences = re.compile('[.!?] ').split(text)\nsentences","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.237925Z","iopub.execute_input":"2024-07-19T20:48:30.238474Z","iopub.status.idle":"2024-07-19T20:48:30.248631Z","shell.execute_reply.started":"2024-07-19T20:48:30.238437Z","shell.execute_reply":"2024-07-19T20:48:30.246367Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3. NLTK","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize,sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.250050Z","iopub.execute_input":"2024-07-19T20:48:30.250440Z","iopub.status.idle":"2024-07-19T20:48:30.262309Z","shell.execute_reply.started":"2024-07-19T20:48:30.250408Z","shell.execute_reply":"2024-07-19T20:48:30.260923Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"sent1 = 'I am going to visit delhi!'\nword_tokenize(sent1)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.263818Z","iopub.execute_input":"2024-07-19T20:48:30.264342Z","iopub.status.idle":"2024-07-19T20:48:30.294764Z","shell.execute_reply.started":"2024-07-19T20:48:30.264291Z","shell.execute_reply":"2024-07-19T20:48:30.293544Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"},"metadata":{}}]},{"cell_type":"code","source":"text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n\nsent_tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.296325Z","iopub.execute_input":"2024-07-19T20:48:30.297359Z","iopub.status.idle":"2024-07-19T20:48:30.306674Z","shell.execute_reply.started":"2024-07-19T20:48:30.297317Z","shell.execute_reply":"2024-07-19T20:48:30.305516Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"},"metadata":{}}]},{"cell_type":"code","source":"sent5 = 'I have a Ph.D in A.I'\nsent6 = \"We're here to help! mail us at nks@gmail.com\" #Failed\nsent7 = 'A 5km ride cost $10.50' #Failed\n\nprint(word_tokenize(sent5))\nprint(word_tokenize(sent6))\nprint(word_tokenize(sent7))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.308163Z","iopub.execute_input":"2024-07-19T20:48:30.308536Z","iopub.status.idle":"2024-07-19T20:48:30.319691Z","shell.execute_reply.started":"2024-07-19T20:48:30.308499Z","shell.execute_reply":"2024-07-19T20:48:30.318596Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n['A', '5km', 'ride', 'cost', '$', '10.50']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4. Spacy","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\ndoc1 = nlp(sent5)\ndoc2 = nlp(sent6)\ndoc3 = nlp(sent7) #Failed\ndoc4 = nlp(sent1)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:30.320976Z","iopub.execute_input":"2024-07-19T20:48:30.321308Z","iopub.status.idle":"2024-07-19T20:48:36.295649Z","shell.execute_reply.started":"2024-07-19T20:48:30.321280Z","shell.execute_reply":"2024-07-19T20:48:36.294548Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"for token in doc3:\n    print(token)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:36.297337Z","iopub.execute_input":"2024-07-19T20:48:36.298280Z","iopub.status.idle":"2024-07-19T20:48:36.303460Z","shell.execute_reply.started":"2024-07-19T20:48:36.298236Z","shell.execute_reply":"2024-07-19T20:48:36.302313Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"A\n5\nkm\nride\ncost\n$\n10.50\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Stemming\n\n#### What is Stemming?\n\nStemming is the process of reducing words to their base or root form. It's like finding the \"stem\" of a word, which can help us understand different variations of the same word.\n\n#### Example:\n\nImagine you have these words:\n\n```plaintext\nrunning, runner, runs, ran\n```\n\nWhen we apply stemming, they all get reduced to the root form:\n\n```plaintext\nrun\n```\n\nSo, \"running\", \"runner\", \"runs\", and \"ran\" all become \"run\".\n\n#### Why Do We Use Stemming in NLP?\n\n1. **Simplifies Text**: Stemming simplifies words to their root form, which makes it easier to analyze text. For instance, \"running\" and \"ran\" are different forms of the same concept, and stemming helps treat them as one.\n\n2. **Reduces Complexity**: By converting different forms of a word to a common base, stemming reduces the number of unique words in a text. This makes the analysis more manageable and less complex.\n\n3. **Improves Search Results**: In tasks like search engines or information retrieval, stemming helps find relevant documents by matching different word forms. For example, searching for \"run\" will also return results for \"running\" and \"ran\".\n\n4. **Consistent Analysis**: It ensures that variations of a word are consistently analyzed together, improving the accuracy of tasks like text classification, sentiment analysis, and topic modeling.\n\n#### Example:\n\nIf you are building a program to understand customer reviews, you might have sentences like:\n\n```plaintext\nI enjoyed running in the park.\nShe runs every morning.\nHe is a fast runner.\nYesterday, I ran for an hour.\n```\n\nStemming will reduce \"running\", \"runs\", \"runner\", and \"ran\" to the common root \"run\". This way, your program understands that all these sentences are about the activity of running.\n\nStemming helps simplify and standardize words in text, making it easier for computers to analyze and understand different forms of words as part of the same concept.\n\n### What is a Stemmer?\n\nA stemmer is a tool in NLP that reduces words to their root form or base form. This helps in simplifying and standardizing words for easier analysis.\n\n### PorterStemmer:\n\n- **Developed by**: Martin Porter in 1980.\n- **Characteristics**: \n  - It's one of the oldest and most widely used stemming algorithms.\n  - It uses a set of rules to iteratively strip suffixes from words.\n  - Known for its simplicity and efficiency.\n- **Example**:\n  ```plaintext\n  \"running\", \"runner\", \"runs\" -> \"run\"\n  ```\n\n### Snowball Stemmer:\n\n- **Developed by**: Martin Porter as well, it's an improvement over the original Porter Stemmer.\n- **Characteristics**:\n  - Also known as the Porter2 Stemmer.\n  - More aggressive and efficient compared to the original Porter Stemmer.\n  - Supports multiple languages, unlike the original Porter Stemmer which is English-only.\n- **Example**:\n  ```plaintext\n  \"running\", \"runner\", \"runs\" -> \"run\"\n  ```\n\nBoth stemmers aim to reduce words to their root form, the Snowball Stemmer is a more advanced and versatile version of the original Porter Stemmer, supporting additional languages and more sophisticated stemming rules.","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\nps = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([ps.stem(word) for word in text.split()])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:36.305073Z","iopub.execute_input":"2024-07-19T20:48:36.305549Z","iopub.status.idle":"2024-07-19T20:48:36.319624Z","shell.execute_reply.started":"2024-07-19T20:48:36.305500Z","shell.execute_reply":"2024-07-19T20:48:36.318589Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"sample = \"running run runs runned\"\nstem_words(sample)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:36.321022Z","iopub.execute_input":"2024-07-19T20:48:36.321968Z","iopub.status.idle":"2024-07-19T20:48:36.334439Z","shell.execute_reply.started":"2024-07-19T20:48:36.321925Z","shell.execute_reply":"2024-07-19T20:48:36.333387Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'run run run run'"},"metadata":{}}]},{"cell_type":"code","source":"text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:36.335782Z","iopub.execute_input":"2024-07-19T20:48:36.336770Z","iopub.status.idle":"2024-07-19T20:48:36.346778Z","shell.execute_reply.started":"2024-07-19T20:48:36.336729Z","shell.execute_reply":"2024-07-19T20:48:36.345565Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n","output_type":"stream"}]},{"cell_type":"code","source":"stem_words(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:36.348054Z","iopub.execute_input":"2024-07-19T20:48:36.348378Z","iopub.status.idle":"2024-07-19T20:48:36.365824Z","shell.execute_reply.started":"2024-07-19T20:48:36.348350Z","shell.execute_reply":"2024-07-19T20:48:36.364568Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Disadvantages of Stemming\n\n1. **Over-Simplification**: Stemming can sometimes be too aggressive, reducing words to forms that are not real words (e.g., \"better\" becoming \"bett\").\n\n2. **Loss of Meaning**: Important nuances and meanings might be lost when words are reduced to their base form (e.g., \"running\" and \"runner\" both becoming \"run\").\n\n3. **Inconsistency**: Different stemming algorithms might produce different results for the same word, leading to inconsistency in text analysis.\n\n4. **Language Limitations**: Some stemmers are designed for specific languages and might not work well with others.\n\nStemming helps in simplifying text, it can sometimes go too far, losing important details and creating inconsistencies.","metadata":{}},{"cell_type":"markdown","source":"## Lemmatization\n\n#### What is Lemmatization?\n\nLemmatization is the process of reducing words to their base or dictionary form, known as the lemma. Unlike stemming, which cuts off word endings, lemmatization considers the context and converts words to their actual root form as found in the dictionary.\n\n#### Example:\n\nImagine you have these words:\n\n```plaintext\nrunning, ran, runs\n```\n\nLemmatization converts them all to:\n\n```plaintext\nrun\n```\n\n#### Why Do We Use Lemmatization in NLP?\n\n1. **Accurate Base Forms**: It provides accurate base forms of words, maintaining the meaning. For example, \"better\" becomes \"good,\" which is its true lemma.\n   \n2. **Improves Understanding**: Helps in understanding the text better by converting words to their proper form, making it easier for NLP models to analyze.\n\n3. **Consistent Analysis**: Ensures consistency in text analysis by using standardized forms of words.\n\n#### What is a Lemma?\n\nA lemma is the base or dictionary form of a word. For instance, the lemma of \"running\" and \"ran\" is \"run.\"\n\nLemmatization is like looking up the correct word form in the dictionary. It helps computers understand and process text more accurately by converting words to their true base form. This way, words like \"running\" and \"ran\" are understood to be the same action, \"run\".","metadata":{}},{"cell_type":"code","source":"import spacy\n\n# Load the small English language model\nnlp = spacy.load('en_core_web_sm')\n\n# Process a text\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n\n# Print named entities\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n    \nprint(\"Word - Lemma\")\nfor token in doc:\n    print(f\"{token.text} - {token.lemma_}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:36.367307Z","iopub.execute_input":"2024-07-19T20:48:36.368076Z","iopub.status.idle":"2024-07-19T20:48:37.236245Z","shell.execute_reply.started":"2024-07-19T20:48:36.368033Z","shell.execute_reply":"2024-07-19T20:48:37.234951Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Apple ORG\nU.K. GPE\n$1 billion MONEY\nWord - Lemma\nApple - Apple\nis - be\nlooking - look\nat - at\nbuying - buy\nU.K. - U.K.\nstartup - startup\nfor - for\n$ - $\n1 - 1\nbillion - billion\n. - .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## When to Use Stemming or Lemmatization?\n\n#### **Stemming:**\n\n- **Quick and Simple**: Use stemming when you need fast results and don't care about perfect accuracy.\n- **Large Datasets**: It's good for handling large amounts of text quickly.\n- **Internal Use**: Best for when the text won't be shown to others, like internal processing or quick keyword matching.\n\n#### **Lemmatization:**\n\n- **Accurate and Contextual**: Use lemmatization for more accurate word forms and better understanding.\n- **Complex Tasks**: Ideal for tasks like sentiment analysis or translation where meaning matters.\n- **Readable Output**: Choose lemmatization when the text will be shown to others, ensuring it looks correct and makes sense.\n\nUse stemming for speed and simplicity when the text is for internal use, and go for lemmatization when accuracy and readability are important.","metadata":{}},{"cell_type":"markdown","source":"## One Hot Encoding\n\n**One-Hot Encoding (OHE)** is a technique used to convert categorical data into a binary (0 or 1) format. Each category in the dataset is transformed into a new binary column, where only one column is set to 1 (indicating the presence of the category), and all others are set to 0.\n\n### Example with a Small Dataset\n\nSuppose we have a dataset with a single categorical feature, \"Fruit\":\n\n| ID | Fruit    |\n|----|----------|\n| 1  | Apple    |\n| 2  | Banana   |\n| 3  | Orange   |\n| 4  | Banana   |\n| 5  | Apple    |\n\nAfter applying one-hot encoding, the dataset is transformed into:\n\n| ID | Fruit_Apple | Fruit_Banana | Fruit_Orange |\n|----|-------------|--------------|--------------|\n| 1  | 1           | 0            | 0            |\n| 2  | 0           | 1            | 0            |\n| 3  | 0           | 0            | 1            |\n| 4  | 0           | 1            | 0            |\n| 5  | 1           | 0            | 0            |\n\n### Advantages of One-Hot Encoding\n\n1. **Simplicity**: Easy to implement and understand.\n2. **No Ordinal Relationships**: Suitable for categorical variables where there is no ordinal relationship (no natural ordering).\n3. **Compatibility**: Works well with many machine learning algorithms, including linear models and neural networks.\n\n### Disadvantages of One-Hot Encoding\n\n1. **High Dimensionality**: Can lead to a large number of columns, especially if the categorical variable has many unique values.\n2. **Sparse Representation**: Results in sparse matrices, which can be memory inefficient.\n3. **Loss of Information**: Does not capture any inherent relationships between categories (e.g., similarity between \"Red\" and \"Pink\").\n4. **Out-of-Vocabulary (OOV)**: Data refers to data that was not present in the training set and therefore not accounted for during the one-hot encoding process. When OOV data appears during the model's deployment or testing phase, it poses significant challenges.\n\n### Why Use One-Hot Encoding in NLP for Feature Extraction\n\nIn Natural Language Processing (NLP), one-hot encoding is often used to represent words or tokens as binary vectors. Hereâ€™s why:\n\n1. **Representation of Categorical Data**: Words are categorical data and need to be converted into a numerical form for machine learning models.\n2. **No Ordinal Relationship**: In many cases, there is no inherent order to words, making one-hot encoding appropriate.\n3. **Compatibility with Algorithms**: Many NLP algorithms and models (e.g., neural networks) can easily work with one-hot encoded vectors.\n4. **Baseline Representation**: One-hot encoding provides a simple baseline representation for more complex embeddings like Word2Vec, GloVe, or contextual embeddings from transformer models.\n\n### Example in NLP\n\nConsider the sentence \"I love NLP\":\n\n| Word  | One-Hot Vector       |\n|-------|-----------------------|\n| I     | [1, 0, 0, 0]          |\n| love  | [0, 1, 0, 0]          |\n| NLP   | [0, 0, 1, 0]          |\n\nEach word is represented as a vector with a length equal to the number of unique words in the vocabulary, with a 1 indicating the presence of the word and 0s elsewhere.\n\n### Conclusion\n\nOne-hot encoding is a fundamental technique in machine learning and NLP for handling categorical data. Despite its limitations, it serves as a simple and effective method for representing categorical features in a format suitable for various algorithms.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.237450Z","iopub.execute_input":"2024-07-19T20:48:37.237816Z","iopub.status.idle":"2024-07-19T20:48:37.242629Z","shell.execute_reply.started":"2024-07-19T20:48:37.237778Z","shell.execute_reply":"2024-07-19T20:48:37.241559Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"data = {\n    'ID': [1, 2, 3, 4, 5],\n    'Fruit': ['Apple', 'Banana', 'Orange', 'Banana', 'Apple']\n}\n\ndf = pd.DataFrame(data)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.244040Z","iopub.execute_input":"2024-07-19T20:48:37.244386Z","iopub.status.idle":"2024-07-19T20:48:37.262454Z","shell.execute_reply.started":"2024-07-19T20:48:37.244357Z","shell.execute_reply":"2024-07-19T20:48:37.261212Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"   ID   Fruit\n0   1   Apple\n1   2  Banana\n2   3  Orange\n3   4  Banana\n4   5   Apple","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Fruit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Apple</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Banana</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Orange</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Banana</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Apple</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"encoder = OneHotEncoder()\nencoded_data = encoder.fit_transform(df[['Fruit']])\nencoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(['Fruit']))\nfinal_df = pd.concat([df, encoded_df], axis=1)\nfinal_df","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.263816Z","iopub.execute_input":"2024-07-19T20:48:37.264210Z","iopub.status.idle":"2024-07-19T20:48:37.291425Z","shell.execute_reply.started":"2024-07-19T20:48:37.264150Z","shell.execute_reply":"2024-07-19T20:48:37.290138Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"   ID   Fruit  Fruit_Apple  Fruit_Banana  Fruit_Orange\n0   1   Apple          1.0           0.0           0.0\n1   2  Banana          0.0           1.0           0.0\n2   3  Orange          0.0           0.0           1.0\n3   4  Banana          0.0           1.0           0.0\n4   5   Apple          1.0           0.0           0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Fruit</th>\n      <th>Fruit_Apple</th>\n      <th>Fruit_Banana</th>\n      <th>Fruit_Orange</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Apple</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Banana</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Orange</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Banana</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Apple</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Bag of Words\n\n**Bag of Words (BoW)** is a simple and commonly used technique in natural language processing (NLP) for converting text data into numerical features. It represents text data by counting the occurrence of each word in the document, disregarding grammar and word order, but keeping multiplicity.\n\n### Example with a Small Dataset\n\nSuppose we have a small dataset with three sentences:\n\n1. \"I love apples\"\n2. \"I hate bananas\"\n3. \"I love oranges\"\n\nFirst, we create a vocabulary of all unique words in the dataset:\n\n| Word     | Index |\n|----------|-------|\n| I        | 1     |\n| love     | 2     |\n| apples   | 3     |\n| hate     | 4     |\n| bananas  | 5     |\n| oranges  | 6     |\n\nUsing this vocabulary, we can represent each sentence as a vector of word counts:\n\n| Sentence         | I | love | apples | hate | bananas | oranges |\n|------------------|---|------|--------|------|---------|---------|\n| \"I love apples\"  | 1 | 1    | 1      | 0    | 0       | 0       |\n| \"I hate bananas\" | 1 | 0    | 0      | 1    | 1       | 0       |\n| \"I love oranges\" | 1 | 1    | 0      | 0    | 0       | 1       |\n\n### Advantages of Bag of Words\n\n1. **Simplicity**: Easy to understand and implement.\n2. **Direct Representation**: Directly represents the frequency of words in a document, making it straightforward to interpret.\n3. **Compatibility**: Works well with many traditional machine learning algorithms like Naive Bayes and Support Vector Machines (SVM).\n\n### Disadvantages of Bag of Words\n\n1. **High Dimensionality**: Can result in very large and sparse matrices, especially with large vocabularies.\n2. **No Context or Semantics**: Ignores the order of words and does not capture any semantic relationships between words.\n3. **Feature Independence**: Assumes independence between words, which is often not true in natural language.\n\n### Why Use Bag of Words in NLP for Feature Extraction\n\n1. **Baseline Model**: Provides a simple baseline for text representation, which can be used as a starting point before moving to more complex models.\n2. **Text Classification**: Effective for text classification tasks where the frequency of individual words is more important than their order or context.\n3. **Feature Extraction**: Converts text into a numerical format that machine learning models can process.\n\n### Core Intuition of Bag of Words\n\nThe core intuition behind Bag of Words is to treat text as a collection of individual words and to represent documents based on the frequency of each word in the document. The representation ignores the order and structure of words, focusing solely on their presence and count.\n\n### Formula\n\nLet \\( D \\) be a document containing words $( w_1, w_2, ..., w_n )$ from a vocabulary $(V)$.\n\nThe Bag of Words representation of $(D)$ can be defined as a vector $(\\mathbf{v}_D)$ where each element $(v_i)$ corresponds to the count of word $(w_i)$ in $(D)$:\n\n$\\mathbf{v}_D = [\\text{count}(w_1, D), \\text{count}(w_2, D), ..., \\text{count}(w_n, D)]$\n\nWhere $\\text{count}(w_i, D)$ is the number of times word $( w_i )$ appears in document $ D $.\n\n### Example\n\nConsider the same sentences:\n\n1. \"I love apples\"\n2. \"I hate bananas\"\n3. \"I love oranges\"\n\nVocabulary: {I, love, apples, hate, bananas, oranges}\n\nFor \"I love apples\":\n\n$\\mathbf{v}_{D1} = [1, 1, 1, 0, 0, 0]$\n\nFor \"I hate bananas\":\n\n$\\mathbf{v}_{D2} = [1, 0, 0, 1, 1, 0]$\n\nFor \"I love oranges\":\n\n$\\mathbf{v}_{D3} = [1, 1, 0, 0, 0, 1]$\n\n### Conclusion\n\nBag of Words is a fundamental technique in NLP for converting text into numerical features. While it has limitations like ignoring word order and context, its simplicity and effectiveness make it a useful starting point for many text processing and machine learning tasks.\n","metadata":{}},{"cell_type":"code","source":"data = {\n    'Text': [\n        'I love apples',\n        'I hate bananas',\n        'I love oranges',\n        'I love mango'\n    ],\n    'Output': [1, 1, 0, 0]\n}\n\ndf = pd.DataFrame(data)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.293224Z","iopub.execute_input":"2024-07-19T20:48:37.294058Z","iopub.status.idle":"2024-07-19T20:48:37.305783Z","shell.execute_reply.started":"2024-07-19T20:48:37.294011Z","shell.execute_reply":"2024-07-19T20:48:37.304462Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"             Text  Output\n0   I love apples       1\n1  I hate bananas       1\n2  I love oranges       0\n3    I love mango       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I love apples</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I hate bananas</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I love oranges</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I love mango</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.307235Z","iopub.execute_input":"2024-07-19T20:48:37.307665Z","iopub.status.idle":"2024-07-19T20:48:37.316749Z","shell.execute_reply.started":"2024-07-19T20:48:37.307628Z","shell.execute_reply":"2024-07-19T20:48:37.315701Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"bow = cv.fit_transform(df['Text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.318305Z","iopub.execute_input":"2024-07-19T20:48:37.318682Z","iopub.status.idle":"2024-07-19T20:48:37.335884Z","shell.execute_reply.started":"2024-07-19T20:48:37.318651Z","shell.execute_reply":"2024-07-19T20:48:37.334716Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"#Unique Words (vocabulary)\ncv.vocabulary_","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.337408Z","iopub.execute_input":"2024-07-19T20:48:37.337783Z","iopub.status.idle":"2024-07-19T20:48:37.349706Z","shell.execute_reply.started":"2024-07-19T20:48:37.337744Z","shell.execute_reply":"2024-07-19T20:48:37.348441Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'love': 3, 'apples': 0, 'hate': 2, 'bananas': 1, 'oranges': 5, 'mango': 4}"},"metadata":{}}]},{"cell_type":"code","source":"bow[0].toarray()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.351304Z","iopub.execute_input":"2024-07-19T20:48:37.351802Z","iopub.status.idle":"2024-07-19T20:48:37.365474Z","shell.execute_reply.started":"2024-07-19T20:48:37.351760Z","shell.execute_reply":"2024-07-19T20:48:37.364175Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, 1, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"vocab = cv.get_feature_names_out()\nvocab","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.367155Z","iopub.execute_input":"2024-07-19T20:48:37.368158Z","iopub.status.idle":"2024-07-19T20:48:37.377718Z","shell.execute_reply.started":"2024-07-19T20:48:37.368123Z","shell.execute_reply":"2024-07-19T20:48:37.376548Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"array(['apples', 'bananas', 'hate', 'love', 'mango', 'oranges'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"bow.toarray()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.378895Z","iopub.execute_input":"2024-07-19T20:48:37.379211Z","iopub.status.idle":"2024-07-19T20:48:37.391469Z","shell.execute_reply.started":"2024-07-19T20:48:37.379186Z","shell.execute_reply":"2024-07-19T20:48:37.390181Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, 1, 0, 0],\n       [0, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1],\n       [0, 0, 0, 1, 1, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"bow_df = pd.DataFrame(bow.toarray(), columns=vocab)\n\nfinal_df = pd.concat([df[['Output']], bow_df], axis=1)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nBag of Words DataFrame with binary parameter as False:\")\nprint(final_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.393169Z","iopub.execute_input":"2024-07-19T20:48:37.394471Z","iopub.status.idle":"2024-07-19T20:48:37.409951Z","shell.execute_reply.started":"2024-07-19T20:48:37.394426Z","shell.execute_reply":"2024-07-19T20:48:37.408859Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Original DataFrame:\n             Text  Output\n0   I love apples       1\n1  I hate bananas       1\n2  I love oranges       0\n3    I love mango       0\n\nBag of Words DataFrame with binary parameter as False:\n   Output  apples  bananas  hate  love  mango  oranges\n0       1       1        0     0     1      0        0\n1       1       0        1     1     0      0        0\n2       0       0        0     0     1      0        1\n3       0       0        0     0     1      1        0\n","output_type":"stream"}]},{"cell_type":"code","source":"#New Text\ncv.transform(['apples apples hate bananas mango']).toarray()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.411248Z","iopub.execute_input":"2024-07-19T20:48:37.411700Z","iopub.status.idle":"2024-07-19T20:48:37.427472Z","shell.execute_reply.started":"2024-07-19T20:48:37.411670Z","shell.execute_reply":"2024-07-19T20:48:37.426378Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"array([[2, 1, 1, 0, 1, 0]])"},"metadata":{}}]},{"cell_type":"markdown","source":"```python\nclass sklearn.feature_extraction.text.CountVectorizer(*, \n                                                      input='content',  # Type of input (string, file, etc.)\n                                                      encoding='utf-8',  # Character encoding for input\n                                                      decode_error='strict',  # Error handling for decoding ('strict', 'ignore','replace')\n                                                      strip_accents=None,  # Remove accents ('ascii', 'unicode', None)\n                                                      lowercase=True,  # Convert text to lowercase\n                                                      preprocessor=None,  # Custom preprocessing function\n                                                      tokenizer=None,  # Custom tokenization function\n                                                      stop_words=None,  # Words to ignore (list or 'english')\n                                                      token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',  # Regex for token extraction\n                                                      ngram_range=(1, 1),  # Range of n-values for n-grams\n                                                      analyzer='word',  # Type of analysis ('word', 'char', 'char_wb')\n                                                      max_df=1.0,  # Max document frequency for filtering\n                                                      min_df=1,  # Min document frequency for filtering\n                                                      max_features=None,  # Max number of features\n                                                      vocabulary=None,  # Predefined vocabulary\n                                                      binary=False,  # If True, return binary occurrence\n                                                      dtype=<class 'numpy.int64'>  # Data type of output\n                                                     )\n\n```\nDoc =https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html","metadata":{}},{"cell_type":"markdown","source":"### N-grams\n\n**N-grams** are contiguous sequences of n items (words, characters, etc.) from a given text or speech. They are used in NLP to capture the context of words by considering the surrounding words in a text.\n\n### Types of N-grams\n\n1. **Unigrams**: Single words (n=1)\n2. **Bigrams**: Pairs of consecutive words (n=2)\n3. **Trigrams**: Triples of consecutive words (n=3)\n4. **N-grams**: General form for sequences of n words\n\n### Example with a Small Dataset\n\nConsider the sentence: \"I love natural language processing\"\n\n**Unigrams** (n=1):\n\n| Unigram |\n|---------|\n| I       |\n| love    |\n| natural |\n| language|\n| processing |\n\n**Bigrams** (n=2):\n\n| Bigram                |\n|-----------------------|\n| I love                |\n| love natural          |\n| natural language      |\n| language processing   |\n\n**Trigrams** (n=3):\n\n| Trigram                     |\n|-----------------------------|\n| I love natural              |\n| love natural language       |\n| natural language processing |\n\n### Advantages of N-grams\n\n1. **Contextual Representation**: Bigrams and trigrams capture more context than unigrams by considering adjacent words.\n2. **Flexibility**: N-grams can be adjusted to capture different levels of context by changing the value of n.\n3. **Improved Performance**: Including bigrams and trigrams can improve the performance of models, especially in tasks like text classification and language modeling.\n\n### Disadvantages of N-grams\n\n1. **Data Sparsity**: As n increases, the number of possible n-grams grows exponentially, leading to sparse data issues.\n2. **Memory and Computation**: Higher-order n-grams require more memory and computational power to process and store.\n3. **Limited Long-range Context**: N-grams capture only a limited context and may miss long-range dependencies in the text.\n\n### Why Use N-grams in NLP for Feature Extraction\n\n1. **Context Preservation**: Unlike unigrams, bigrams and trigrams can preserve some word order and context information, which is crucial for many NLP tasks.\n2. **Feature Enrichment**: Using n-grams enriches the feature set for machine learning models, potentially leading to better performance.\n3. **Language Models**: N-grams are foundational in building statistical language models to predict the next word in a sequence.\n\n### Core Intuition of N-grams\n\nThe core intuition behind n-grams is to capture local context by considering contiguous sequences of words. This helps in understanding the syntactic and semantic relationships between words, improving the model's ability to process natural language.\n\n### Formula\n\nFor a given sequence of words $(S)$ = $([w_1, w_2, \\ldots, w_m])$:\n\n1. **Unigram**: $(S)$ itself as $([w_1, w_2, \\ldots, w_m])$\n2. **Bigram**: $((w_1, w_2), (w_2, w_3), \\ldots, (w_{m-1}, w_m))$\n3. **Trigram**: $((w_1, w_2, w_3), (w_2, w_3, w_4), \\ldots, (w_{m-2}, w_{m-1}, w_m))$\n\n### Example\n\nConsider the same sentence: \"I love natural language processing\"\n\n**Unigrams**:\n\n$ S_{uni} = [I, love, natural, language, processing] $\n\n**Bigrams**:\n\n$ S_{bi} = [(I, love), (love, natural), (natural, language), (language, processing)] $\n\n**Trigrams**:\n\n$ S_{tri} = [(I, love, natural), (love, natural, language), (natural, language, processing)] $\n\n### Conclusion\n\nN-grams are a fundamental concept in NLP for capturing the local context of words in a text. By considering sequences of words, n-grams help in preserving some syntactic and semantic relationships, making them useful for various NLP tasks such as text classification, language modeling, and feature extraction. Despite their limitations, n-grams provide a simple yet powerful way to enhance text representation and improve model performance.","metadata":{}},{"cell_type":"code","source":"data = {\n    'Text': [\n        'I love apples',\n        'I hate bananas',\n        'I love oranges',\n        'I love mango'\n    ],\n    'Output': [1, 1, 0, 0]\n}\n\ndf = pd.DataFrame(data)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.428744Z","iopub.execute_input":"2024-07-19T20:48:37.429094Z","iopub.status.idle":"2024-07-19T20:48:37.443882Z","shell.execute_reply.started":"2024-07-19T20:48:37.429064Z","shell.execute_reply":"2024-07-19T20:48:37.442653Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"             Text  Output\n0   I love apples       1\n1  I hate bananas       1\n2  I love oranges       0\n3    I love mango       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I love apples</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I hate bananas</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I love oranges</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I love mango</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range = (1,2)) #Both Uni and Bi gram","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.445530Z","iopub.execute_input":"2024-07-19T20:48:37.445923Z","iopub.status.idle":"2024-07-19T20:48:37.454031Z","shell.execute_reply.started":"2024-07-19T20:48:37.445871Z","shell.execute_reply":"2024-07-19T20:48:37.452752Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"gram = cv.fit_transform(df['Text'])\nfeature_name = cv.get_feature_names_out()\ndf_gram = pd.DataFrame(gram.toarray(), columns = feature_name)\ndf_gram","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.455425Z","iopub.execute_input":"2024-07-19T20:48:37.455842Z","iopub.status.idle":"2024-07-19T20:48:37.474133Z","shell.execute_reply.started":"2024-07-19T20:48:37.455812Z","shell.execute_reply":"2024-07-19T20:48:37.472831Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"   apples  bananas  hate  hate bananas  love  love apples  love mango  \\\n0       1        0     0             0     1            1           0   \n1       0        1     1             1     0            0           0   \n2       0        0     0             0     1            0           0   \n3       0        0     0             0     1            0           1   \n\n   love oranges  mango  oranges  \n0             0      0        0  \n1             0      0        0  \n2             1      0        1  \n3             0      1        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>apples</th>\n      <th>bananas</th>\n      <th>hate</th>\n      <th>hate bananas</th>\n      <th>love</th>\n      <th>love apples</th>\n      <th>love mango</th>\n      <th>love oranges</th>\n      <th>mango</th>\n      <th>oranges</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(cv.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:48:37.475849Z","iopub.execute_input":"2024-07-19T20:48:37.476387Z","iopub.status.idle":"2024-07-19T20:48:37.484773Z","shell.execute_reply.started":"2024-07-19T20:48:37.476348Z","shell.execute_reply":"2024-07-19T20:48:37.483734Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"markdown","source":"## TF-IDF\n\n**TF-IDF (Term Frequency-Inverse Document Frequency)** is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It combines two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).\n\n- **Term Frequency (TF)**: Measures how frequently a term occurs in a document. It is often normalized to prevent bias towards longer documents.\n- **Inverse Document Frequency (IDF)**: Measures how important a term is. It decreases the weight of terms that appear frequently in many documents and increases the weight of terms that appear in a few documents.\n\n### Example\n\nConsider a corpus with three documents:\n\n1. \"I love apples\"\n2. \"I love bananas\"\n3. \"I love apples and bananas\"\n\n#### Step 1: Calculate Term Frequency (TF)\n\n$$\n\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n$$\n\n| Document | Term     | TF      |\n|----------|----------|---------|\n| D1       | I        | 1/3     |\n| D1       | love     | 1/3     |\n| D1       | apples   | 1/3     |\n| D2       | I        | 1/3     |\n| D2       | love     | 1/3     |\n| D2       | bananas  | 1/3     |\n| D3       | I        | 1/5     |\n| D3       | love     | 1/5     |\n| D3       | apples   | 1/5     |\n| D3       | and      | 1/5     |\n| D3       | bananas  | 1/5     |\n\n#### Step 2: Calculate Inverse Document Frequency (IDF)\n\n$$\n\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)\n$$\n\nIDF is calculated using the formula:\n$text{IDF}(t) = \\log \\left(\\frac{N}{df(t)}\\right) $\nwhere $ N $ is the total number of documents, and $ df(t) $ is the number of documents containing the term $ t $.\n\n| Term    | df(t) | IDF       |\n|---------|-------|-----------|\n| I       | 3     | log(3/3) = 0    |\n| love    | 3     | log(3/3) = 0    |\n| apples  | 2     | log(3/2) = 0.176|\n| bananas | 2     | log(3/2) = 0.176|\n| and     | 1     | log(3/1) = 0.477|\n\n#### Step 3: Calculate TF-IDF\n\nTF-IDF is calculated by multiplying TF and IDF for each term in each document.\n\n| Document | Term     | TF      | IDF       | TF-IDF        |\n|----------|----------|---------|-----------|---------------|\n| D1       | I        | 1/3     | 0         | 0             |\n| D1       | love     | 1/3     | 0         | 0             |\n| D1       | apples   | 1/3     | 0.176     | 0.059         |\n| D2       | I        | 1/3     | 0         | 0             |\n| D2       | love     | 1/3     | 0         | 0             |\n| D2       | bananas  | 1/3     | 0.176     | 0.059         |\n| D3       | I        | 1/5     | 0         | 0             |\n| D3       | love     | 1/5     | 0         | 0             |\n| D3       | apples   | 1/5     | 0.176     | 0.035         |\n| D3       | and      | 1/5     | 0.477     | 0.095         |\n| D3       | bananas  | 1/5     | 0.176     | 0.035         |\n\n### Advantages of TF-IDF\n\n1. **Simple to Understand**: TF-IDF is easy to compute and understand.\n2. **Effective**: Often effective for text representation in various NLP tasks like text classification and information retrieval.\n3. **Reduces Noise**: By reducing the weight of common terms, it helps in emphasizing more informative words.\n\n### Disadvantages of TF-IDF\n\n1. **High Dimensionality**: Like Bag of Words, it can result in high-dimensional feature vectors.\n2. **Context Ignorance**: Does not capture the semantic meaning or context of words.\n3. **Static Nature**: Needs to be recomputed if the corpus changes, which can be computationally expensive for large datasets.\n\n### Why Use TF-IDF in NLP for Feature Extraction\n\n1. **Feature Weighting**: Provides a way to weight features based on their importance, making it useful for text mining and information retrieval.\n2. **Information Retrieval**: Helps in ranking documents based on relevance to a query by giving higher weight to rare but important terms.\n3. **Text Classification**: Enhances the performance of classifiers by focusing on significant words and ignoring common words that carry less information.\n\n### Core Intuition of TF-IDF\n\nThe core intuition behind TF-IDF is to assign higher weights to words that are important in a particular document but not common across the entire corpus. This helps in distinguishing documents based on their unique terms.\n\n### TF-IDF Formula\n\nThe TF-IDF value for a term $ t $ in a document $ d $ is calculated as:\n\n$ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t) $\n\nwhere:\n- $ \\text{TF}(t, d) $ is the term frequency of $ t $ in document $ d $.\n- $ \\text{IDF}(t) $ is the inverse document frequency of term $ t $.\n\n### Example\n\nConsider the sentence \"I love apples\":\n\n1. **Term Frequency (TF)**: \n\n| Term   | TF (raw count) | TF (normalized) |\n|--------|----------------|-----------------|\n| I      | 1              | 1/3             |\n| love   | 1              | 1/3             |\n| apples | 1              | 1/3             |\n\n2. **Inverse Document Frequency (IDF)**:\n\n| Term   | df(t) | IDF                  |\n|--------|-------|----------------------|\n| I      | 3     | log(3/3) = 0    |\n| love   | 3     | log(3/3) = 0    |\n| apples | 2     | log(3/2) = 0.176|\n\n3. **TF-IDF Calculation**:\n\n| Term   | TF (normalized) | IDF      | TF-IDF       |\n|--------|-----------------|----------|--------------|\n| I      | 1/3             | 0        | 0            |\n| love   | 1/3             | 0        | 0            |\n| apples | 1/3             | 0.176    | 0.059        |\n\n### Conclusion\n\nTF-IDF is a powerful technique for text feature extraction, offering a way to weigh terms by their importance in a document relative to a corpus. It reduces the influence of common terms and highlights unique terms, improving the effectiveness of information retrieval and text classification tasks.","metadata":{}},{"cell_type":"code","source":"data = {\n    'Text': [\n        'I love apples',\n        'I hate bananas',\n        'I love oranges',\n        'I love mango'\n    ],\n    'Output': [1, 1, 0, 0]\n}\n\ndf = pd.DataFrame(data)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:03:30.428219Z","iopub.execute_input":"2024-07-19T21:03:30.428681Z","iopub.status.idle":"2024-07-19T21:03:30.440962Z","shell.execute_reply.started":"2024-07-19T21:03:30.428650Z","shell.execute_reply":"2024-07-19T21:03:30.439868Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"             Text  Output\n0   I love apples       1\n1  I hate bananas       1\n2  I love oranges       0\n3    I love mango       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I love apples</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I hate bananas</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I love oranges</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I love mango</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nout = tfidf.fit_transform(df['Text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:16:52.382132Z","iopub.execute_input":"2024-07-19T21:16:52.382565Z","iopub.status.idle":"2024-07-19T21:16:52.392668Z","shell.execute_reply.started":"2024-07-19T21:16:52.382531Z","shell.execute_reply":"2024-07-19T21:16:52.391333Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"print(tfidf.idf_)\nprint(tfidf.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:16:54.592396Z","iopub.execute_input":"2024-07-19T21:16:54.593462Z","iopub.status.idle":"2024-07-19T21:16:54.599401Z","shell.execute_reply.started":"2024-07-19T21:16:54.593424Z","shell.execute_reply":"2024-07-19T21:16:54.598203Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"[1.91629073 1.91629073 1.91629073 1.22314355 1.91629073 1.91629073]\n['apples' 'bananas' 'hate' 'love' 'mango' 'oranges']\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_name = tfidf.get_feature_names_out()\ndf_tfidf = pd.DataFrame(out.toarray(), columns = feature_name)\ndf_final = pd.concat([df[['Text']], df_tfidf], axis = 1)\ndf_final","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:19:37.881091Z","iopub.execute_input":"2024-07-19T21:19:37.881509Z","iopub.status.idle":"2024-07-19T21:19:37.897741Z","shell.execute_reply.started":"2024-07-19T21:19:37.881458Z","shell.execute_reply":"2024-07-19T21:19:37.896499Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"             Text    apples   bananas      hate      love     mango   oranges\n0   I love apples  0.842926  0.000000  0.000000  0.538029  0.000000  0.000000\n1  I hate bananas  0.000000  0.707107  0.707107  0.000000  0.000000  0.000000\n2  I love oranges  0.000000  0.000000  0.000000  0.538029  0.000000  0.842926\n3    I love mango  0.000000  0.000000  0.000000  0.538029  0.842926  0.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>apples</th>\n      <th>bananas</th>\n      <th>hate</th>\n      <th>love</th>\n      <th>mango</th>\n      <th>oranges</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I love apples</td>\n      <td>0.842926</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.538029</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I hate bananas</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I love oranges</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.538029</td>\n      <td>0.000000</td>\n      <td>0.842926</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I love mango</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.538029</td>\n      <td>0.842926</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}